{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2l09AGRWdQPP",
        "-CbWbFJMc9Xu",
        "1kfr9GHvAtqG"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3994fd4714174bfba8c9aec71aa61faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50d38cc72e434e779ae920108b78e0f6",
              "IPY_MODEL_e4df76fe7c004a33b6fb6debdb4a19ef",
              "IPY_MODEL_3d5ee9904e3e4974aa2a3bd80e3ccaf6"
            ],
            "layout": "IPY_MODEL_e5012bf89e9c484f86deefde708b24bd"
          }
        },
        "50d38cc72e434e779ae920108b78e0f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_802db505ebff4e0ba803717d9efc11db",
            "placeholder": "​",
            "style": "IPY_MODEL_494ca2d0cbaa4bb09a01d601de9e6dc8",
            "value": "Loading weights: 100%"
          }
        },
        "e4df76fe7c004a33b6fb6debdb4a19ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24cf0c650fec47dd9e69e5c4edf8c7be",
            "max": 1172,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0438655e6e24077acf8db0e0421c8c2",
            "value": 1172
          }
        },
        "3d5ee9904e3e4974aa2a3bd80e3ccaf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f90f95c6652d4f51bb47ae8a9e8b6c06",
            "placeholder": "​",
            "style": "IPY_MODEL_faaafed6b947440c80dc5cdb0a63590d",
            "value": " 1172/1172 [00:10&lt;00:00, 812.08it/s, Materializing param=segformer.encoder.patch_embeddings.3.proj.weight]"
          }
        },
        "e5012bf89e9c484f86deefde708b24bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802db505ebff4e0ba803717d9efc11db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494ca2d0cbaa4bb09a01d601de9e6dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24cf0c650fec47dd9e69e5c4edf8c7be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0438655e6e24077acf8db0e0421c8c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f90f95c6652d4f51bb47ae8a9e8b6c06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faaafed6b947440c80dc5cdb0a63590d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cb892c983aa4bc2a9a1423d42dc62a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e12294c0375460dbf13b69337c9f08e",
              "IPY_MODEL_d3e72944ba4844aaa11d85d4e548445a",
              "IPY_MODEL_525bcf56d14b4f22b8b41195b71a40e9"
            ],
            "layout": "IPY_MODEL_8c46f24cb3bb40c9949c1c389d2f6e9a"
          }
        },
        "8e12294c0375460dbf13b69337c9f08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d50e9bb7e56a402f91e849149259fc2e",
            "placeholder": "​",
            "style": "IPY_MODEL_f89398d0d5b7486aa99687870097fe84",
            "value": "Loading weights: 100%"
          }
        },
        "d3e72944ba4844aaa11d85d4e548445a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03960145c83143ba995d65ca4233d9d3",
            "max": 1172,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5e41e1c8ad44c0a8e1df8b312f1b66a",
            "value": 1172
          }
        },
        "525bcf56d14b4f22b8b41195b71a40e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f053d54dc8b7482a8c345985ff62eb12",
            "placeholder": "​",
            "style": "IPY_MODEL_230f5266ebd041c58caae879f7bee096",
            "value": " 1172/1172 [00:01&lt;00:00, 720.34it/s, Materializing param=segformer.encoder.patch_embeddings.3.proj.weight]"
          }
        },
        "8c46f24cb3bb40c9949c1c389d2f6e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d50e9bb7e56a402f91e849149259fc2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f89398d0d5b7486aa99687870097fe84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03960145c83143ba995d65ca4233d9d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5e41e1c8ad44c0a8e1df8b312f1b66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f053d54dc8b7482a8c345985ff62eb12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "230f5266ebd041c58caae879f7bee096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install supabase\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3TfpgKCqi7m",
        "outputId": "fd8ae5a9-98b6-45b6-d082-2d630dd33b91",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rIfwJpn0XVpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integration"
      ],
      "metadata": {
        "id": "2l09AGRWdQPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SUPABASE_URL\"] = \"https://viqawhbbhlkbjxntmwpn.supabase.co\"\n",
        "os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InZpcWF3aGJiaGxrYmp4bnRtd3BuIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTYzNjEwMywiZXhwIjoyMDg1MjEyMTAzfQ.DvUNXsw8gwdxhiG71QPYNVFhQIYRcDQuP9QC2VOiCu0\"  # backend-only\n"
      ],
      "metadata": {
        "id": "ZKRjz7q9qmGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from supabase import create_client\n",
        "\n",
        "# 1) Force trailing slash\n",
        "os.environ[\"SUPABASE_URL\"] = os.environ[\"SUPABASE_URL\"].strip()\n",
        "if not os.environ[\"SUPABASE_URL\"].endswith(\"/\"):\n",
        "    os.environ[\"SUPABASE_URL\"] += \"/\"\n",
        "\n",
        "# 2) Recreate client AFTER fixing\n",
        "supabase = create_client(os.environ[\"SUPABASE_URL\"], os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"])\n",
        "\n",
        "print(\"OK URL:\", os.environ[\"SUPABASE_URL\"])\n",
        "print(\"OK client:\", supabase.supabase_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXFJKdpbrwQO",
        "outputId": "6026a83a-d1e8-4087-de1d-cda11b161159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK URL: https://viqawhbbhlkbjxntmwpn.supabase.co/\n",
            "OK client: https://viqawhbbhlkbjxntmwpn.supabase.co/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check DB tables exist\n",
        "try:\n",
        "    r = supabase.table(\"scene_runs\").select(\"id\").limit(1).execute()\n",
        "    r2 = supabase.table(\"pipeline_outputs\").select(\"id\").limit(1).execute()\n",
        "    print(\"OK DB: scene_runs + pipeline_outputs exist\")\n",
        "except Exception as e:\n",
        "    print(\"DB FAIL:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPpo1IWxsUib",
        "outputId": "97eae1e6-77c4-49ff-b9a2-059ae821199b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK DB: scene_runs + pipeline_outputs exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET = \"Pipeline\"\n",
        "\n",
        "test_path = \"runs/_debug/test.txt\"\n",
        "content = b\"hello supabase\"\n",
        "\n",
        "supabase.storage.from_(BUCKET).upload(\n",
        "    path=test_path,\n",
        "    file=content,\n",
        "    file_options={\"content-type\":\"text/plain\",\"upsert\":\"true\"}\n",
        ")\n",
        "\n",
        "print(\"OK Storage upload:\", test_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvZjXbyJtNsY",
        "outputId": "413f4a0e-6ab8-483b-c2a9-66dceaf73313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK Storage upload: runs/_debug/test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, os\n",
        "\n",
        "BUCKET = \"Pipeline\"\n",
        "\n",
        "IMG_PATH = \"/content/drive/MyDrive/Depthanythingv2/data/eval/402db54d-53c8-4392-a5dc-b2e5fb6c337a.jpg\"\n",
        "\n",
        "# 1) Create run\n",
        "run = supabase.table(\"scene_runs\").insert({\n",
        "    \"scene_name\": \"debug_depth_run\",\n",
        "    \"notes\": \"step4: input upload only\"\n",
        "}).execute()\n",
        "\n",
        "run_id = run.data[0][\"id\"]\n",
        "print(\"OK run_id:\", run_id)\n",
        "\n",
        "# 2) Read image\n",
        "img = cv2.imread(IMG_PATH)\n",
        "if img is None:\n",
        "    raise ValueError(\"Image not found/readable: \" + IMG_PATH)\n",
        "\n",
        "# 3) Upload image to storage under run folder\n",
        "input_path = f\"runs/{run_id}/input/original.jpg\"\n",
        "\n",
        "ok, buf = cv2.imencode(\".jpg\", img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
        "if not ok:\n",
        "    raise RuntimeError(\"JPEG encode failed\")\n",
        "\n",
        "supabase.storage.from_(BUCKET).upload(\n",
        "    path=input_path,\n",
        "    file=buf.tobytes(),\n",
        "    file_options={\"content-type\":\"image/jpeg\",\"upsert\":\"true\"}\n",
        ")\n",
        "print(\"OK storage input upload:\", input_path)\n",
        "\n",
        "# 4) Upsert DB row in pipeline_outputs\n",
        "h, w = img.shape[:2]\n",
        "row = {\n",
        "    \"run_id\": run_id,\n",
        "    \"stage\": \"input\",\n",
        "    \"file_role\": \"original_image\",\n",
        "    \"bucket\": BUCKET,\n",
        "    \"path\": input_path,\n",
        "    \"mime_type\": \"image/jpeg\",\n",
        "    \"meta\": {\"width\": w, \"height\": h, \"source\": \"colab_step4\"}\n",
        "}\n",
        "supabase.table(\"pipeline_outputs\").upsert(row).execute()\n",
        "\n",
        "print(\"OK DB row upserted for input\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We56Mf-ktPlH",
        "outputId": "9ef67351-af59-4666-9a83-e0d3ac035773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK run_id: aea38f28-476b-4d89-8cc7-707b83611ef3\n",
            "OK storage input upload: runs/aea38f28-476b-4d89-8cc7-707b83611ef3/input/original.jpg\n",
            "OK DB row upserted for input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"letterbox:\", \"letterbox\" in globals())\n",
        "print(\"unletterbox:\", \"unletterbox\" in globals())\n",
        "print(\"infer_tta_rel:\", \"infer_tta_rel\" in globals())\n",
        "print(\"to_u16_rel:\", \"to_u16_rel\" in globals())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHwiHqdxtiCF",
        "outputId": "b91c01f3-b841-4546-dcb1-597a76ec1b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "letterbox: False\n",
            "unletterbox: False\n",
            "infer_tta_rel: False\n",
            "to_u16_rel: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IROmRmostiJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, cv2, numpy as np, torch\n",
        "\n",
        "# --- paths: update if yours differ ---\n",
        "REPO = \"/content/drive/MyDrive/Depthanythingv2\"\n",
        "CKPT = f\"{REPO}/checkpoints/depth_anything_v2_vitl.pth\"\n",
        "\n",
        "# Make sure we can import the library\n",
        "if REPO not in sys.path:\n",
        "    sys.path.append(REPO)\n",
        "\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# vitl config\n",
        "model = DepthAnythingV2(encoder=\"vitl\", features=256, out_channels=[256, 512, 1024, 1024])\n",
        "state = torch.load(CKPT, map_location=\"cpu\")\n",
        "model.load_state_dict(state, strict=True)\n",
        "model = model.to(DEVICE).eval()\n",
        "print(\"OK model loaded on\", DEVICE)\n",
        "\n",
        "# ---- required helpers ----\n",
        "def letterbox(img, target=896):\n",
        "    h, w = img.shape[:2]\n",
        "    s = target / max(h, w)\n",
        "    nh, nw = int(round(h * s)), int(round(w * s))\n",
        "    img_r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_CUBIC)\n",
        "    top = (target - nh) // 2; bottom = target - nh - top\n",
        "    left = (target - nw) // 2; right = target - nw - left\n",
        "    img_p = cv2.copyMakeBorder(img_r, top, bottom, left, right,\n",
        "                               cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "    return img_p, (top, bottom, left, right), (h, w)\n",
        "\n",
        "def unletterbox(arr, pads, orig_hw):\n",
        "    top, bottom, left, right = pads\n",
        "    arr = arr[top:arr.shape[0]-bottom, left:arr.shape[1]-right]\n",
        "    return cv2.resize(arr, (orig_hw[1], orig_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_tta_rel(img_bgr):\n",
        "    # Predict and average normal + flipped; keep float32\n",
        "    d0 = model.infer_image(img_bgr).astype(np.float32)\n",
        "    d1 = model.infer_image(cv2.flip(img_bgr, 1)).astype(np.float32)\n",
        "    d1 = np.flip(d1, axis=1)\n",
        "    return (0.5 * (d0 + d1)).astype(np.float32)\n",
        "\n",
        "def to_u16_rel(depth):\n",
        "    d = depth.astype(np.float32)\n",
        "    d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    if (d > 0).sum() < 10:\n",
        "        return np.zeros_like(d, dtype=np.uint16)\n",
        "    p1, p99 = np.percentile(d[d > 0], (1, 99))\n",
        "    p1 = np.float32(p1); p99 = np.float32(p99)\n",
        "    d = np.clip(d, p1, p99)\n",
        "    d = (d - p1) / max(np.float32(1e-6), (p99 - p1))\n",
        "    d = np.clip(d, np.float32(1e-6), np.float32(1.0))\n",
        "    return (d * np.float32(65535.0)).astype(np.uint16)\n",
        "\n",
        "print(\"OK helpers defined: letterbox/unletterbox/infer_tta_rel/to_u16_rel\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta3PYjk8q1jg",
        "outputId": "76565b99-535c-4d8e-df8a-1fe2d6138582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK model loaded on cuda\n",
            "OK helpers defined: letterbox/unletterbox/infer_tta_rel/to_u16_rel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, numpy as np\n",
        "\n",
        "BUCKET = \"Pipeline\"\n",
        "IMG_PATH = \"/content/drive/MyDrive/Depthanythingv2/data/eval/c2dfaff7-cab9-4f3c-9a83-71d88d36526c.JPG\"\n",
        "\n",
        "# 1) Create run\n",
        "run = supabase.table(\"scene_runs\").insert({\n",
        "    \"scene_name\": \"depth_trial\",\n",
        "    \"notes\": \"step5: depth infer + upload\"\n",
        "}).execute()\n",
        "run_id = run.data[0][\"id\"]\n",
        "print(\"OK run_id:\", run_id)\n",
        "\n",
        "# 2) Read image\n",
        "img = cv2.imread(IMG_PATH)\n",
        "if img is None:\n",
        "    raise ValueError(\"Image not found/readable: \" + IMG_PATH)\n",
        "h, w = img.shape[:2]\n",
        "\n",
        "# 3) Upload input\n",
        "input_path = f\"runs/{run_id}/input/original.jpg\"\n",
        "ok, buf = cv2.imencode(\".jpg\", img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
        "if not ok:\n",
        "    raise RuntimeError(\"JPEG encode failed\")\n",
        "\n",
        "supabase.storage.from_(BUCKET).upload(\n",
        "    path=input_path,\n",
        "    file=buf.tobytes(),\n",
        "    file_options={\"content-type\":\"image/jpeg\",\"upsert\":\"true\"}\n",
        ")\n",
        "\n",
        "supabase.table(\"pipeline_outputs\").upsert({\n",
        "    \"run_id\": run_id,\n",
        "    \"stage\": \"input\",\n",
        "    \"file_role\": \"original_image\",\n",
        "    \"bucket\": BUCKET,\n",
        "    \"path\": input_path,\n",
        "    \"mime_type\": \"image/jpeg\",\n",
        "    \"meta\": {\"width\": w, \"height\": h}\n",
        "}).execute()\n",
        "print(\"OK input stored + DB row\")\n",
        "\n",
        "# 4) Depth inference\n",
        "img_p, pads, orig_hw = letterbox(img, 896)\n",
        "depth_rel_p = infer_tta_rel(img_p)\n",
        "depth_rel = unletterbox(depth_rel_p, pads, orig_hw).astype(np.float32)\n",
        "\n",
        "rel16 = to_u16_rel(depth_rel)\n",
        "viz = cv2.applyColorMap(255 - (rel16 // 256).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
        "print(\"OK depth inferred\")\n",
        "\n",
        "# 5) Upload depth outputs\n",
        "depth_png_path = f\"runs/{run_id}/depth/depth_rel_u16.png\"\n",
        "depth_viz_path = f\"runs/{run_id}/depth/depth_viz.jpg\"\n",
        "\n",
        "ok, buf_png = cv2.imencode(\".png\", rel16)\n",
        "if not ok:\n",
        "    raise RuntimeError(\"Depth PNG encode failed\")\n",
        "ok, buf_viz = cv2.imencode(\".jpg\", viz, [int(cv2.IMWRITE_JPEG_QUALITY), 92])\n",
        "if not ok:\n",
        "    raise RuntimeError(\"Viz JPG encode failed\")\n",
        "\n",
        "supabase.storage.from_(BUCKET).upload(\n",
        "    path=depth_png_path,\n",
        "    file=buf_png.tobytes(),\n",
        "    file_options={\"content-type\":\"image/png\",\"upsert\":\"true\"}\n",
        ")\n",
        "supabase.storage.from_(BUCKET).upload(\n",
        "    path=depth_viz_path,\n",
        "    file=buf_viz.tobytes(),\n",
        "    file_options={\"content-type\":\"image/jpeg\",\"upsert\":\"true\"}\n",
        ")\n",
        "\n",
        "# 6) DB upserts for outputs\n",
        "supabase.table(\"pipeline_outputs\").upsert({\n",
        "    \"run_id\": run_id,\n",
        "    \"stage\": \"depth\",\n",
        "    \"file_role\": \"depth_map_png16\",\n",
        "    \"bucket\": BUCKET,\n",
        "    \"path\": depth_png_path,\n",
        "    \"mime_type\": \"image/png\",\n",
        "    \"meta\": {\"encoding\":\"u16_rel\", \"model\":\"DA2_vitl\", \"tta\": True, \"input_size\": 896}\n",
        "}).execute()\n",
        "\n",
        "supabase.table(\"pipeline_outputs\").upsert({\n",
        "    \"run_id\": run_id,\n",
        "    \"stage\": \"depth\",\n",
        "    \"file_role\": \"depth_viz\",\n",
        "    \"bucket\": BUCKET,\n",
        "    \"path\": depth_viz_path,\n",
        "    \"mime_type\": \"image/jpeg\",\n",
        "    \"meta\": {\"colormap\":\"inferno\"}\n",
        "}).execute()\n",
        "\n",
        "print(\"OK depth outputs uploaded + DB rows\")\n",
        "print(\"RUN DONE:\", run_id)\n",
        "print(\"Input:\", input_path)\n",
        "print(\"Depth PNG:\", depth_png_path)\n",
        "print(\"Depth Viz:\", depth_viz_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaHgmYqJrI74",
        "outputId": "cadde03d-6de5-49a4-b075-acbd21726e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK run_id: 13a3f2a8-a2f7-4b2f-95ff-f08d662f357e\n",
            "OK input stored + DB row\n",
            "OK depth inferred\n",
            "OK depth outputs uploaded + DB rows\n",
            "RUN DONE: 13a3f2a8-a2f7-4b2f-95ff-f08d662f357e\n",
            "Input: runs/13a3f2a8-a2f7-4b2f-95ff-f08d662f357e/input/original.jpg\n",
            "Depth PNG: runs/13a3f2a8-a2f7-4b2f-95ff-f08d662f357e/depth/depth_rel_u16.png\n",
            "Depth Viz: runs/13a3f2a8-a2f7-4b2f-95ff-f08d662f357e/depth/depth_viz.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating with our actual erd"
      ],
      "metadata": {
        "id": "-CbWbFJMc9Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SUPABASE_URL\"] = \"https://gkfrvfepccknnoigicxz.supabase.co\"\n",
        "os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdrZnJ2ZmVwY2Nrbm5vaWdpY3h6Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTY4MzkxNCwiZXhwIjoyMDg1MjU5OTE0fQ.T9qJ1h4vQinNy6A9obcNW35Np945tTa3t-u_ZmMtWWo\"  # backend-only\n"
      ],
      "metadata": {
        "id": "Dhfr7otndLTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, cv2, numpy as np, torch\n",
        "from supabase import create_client\n",
        "\n",
        "# ---------- Supabase init ----------\n",
        "os.environ[\"SUPABASE_URL\"] = os.environ[\"SUPABASE_URL\"].strip()\n",
        "if not os.environ[\"SUPABASE_URL\"].endswith(\"/\"):\n",
        "    os.environ[\"SUPABASE_URL\"] += \"/\"\n",
        "\n",
        "supabase = create_client(os.environ[\"SUPABASE_URL\"], os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"])"
      ],
      "metadata": {
        "id": "Fu6FUrPjdGAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET = \"pipeline\"  # <- pick ONE, lowercase recommended\n",
        "\n",
        "def upload_bytes(path, content_bytes, content_type):\n",
        "    supabase.storage.from_(BUCKET).upload(\n",
        "        path=path,\n",
        "        file=content_bytes,\n",
        "        file_options={\"content-type\": content_type, \"upsert\": \"true\"}\n",
        "    )\n",
        "\n",
        "def encode_jpg(bgr, quality=95):\n",
        "    ok, buf = cv2.imencode(\".jpg\", bgr, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
        "    if not ok: raise RuntimeError(\"JPEG encode failed\")\n",
        "    return buf.tobytes()\n",
        "\n",
        "def encode_png_u16(u16):\n",
        "    ok, buf = cv2.imencode(\".png\", u16)\n",
        "    if not ok: raise RuntimeError(\"PNG encode failed\")\n",
        "    return buf.tobytes()"
      ],
      "metadata": {
        "id": "-ZnWKjMMdkaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os, glob\n",
        "\n",
        "REPO = \"/content/drive/MyDrive/Depthanythingv2\"\n",
        "dirs = glob.glob(REPO + \"/**/depth_anything_v2\", recursive=True)\n",
        "\n",
        "if not dirs:\n",
        "    raise RuntimeError(\"Can't find depth_anything_v2 folder anywhere under REPO.\")\n",
        "\n",
        "module_dir = dirs[0]                      # .../depth_anything_v2\n",
        "repo_root = os.path.dirname(module_dir)   # the folder that CONTAINS depth_anything_v2\n",
        "\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "\n",
        "print(\"Using repo_root:\", repo_root)\n",
        "print(\"sys.path[0]:\", sys.path[0])\n",
        "\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "print(\"OK import DepthAnythingV2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_mbQ8EnefKw",
        "outputId": "16400f45-a27a-4feb-aa2e-7ad4f2592969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using repo_root: /content/drive/MyDrive/Depthanythingv2/Depth-Anything-V2\n",
            "sys.path[0]: /content/drive/MyDrive/Depthanythingv2/Depth-Anything-V2\n",
            "OK import DepthAnythingV2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- DepthAnythingV2 load ----------\n",
        "REPO = \"/content/drive/MyDrive/Depthanythingv2\"\n",
        "CKPT = f\"{REPO}/checkpoints/depth_anything_v2_vitl.pth\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = DepthAnythingV2(encoder=\"vitl\", features=256, out_channels=[256, 512, 1024, 1024])\n",
        "state = torch.load(CKPT, map_location=\"cpu\")\n",
        "model.load_state_dict(state, strict=True)\n",
        "model = model.to(DEVICE).eval()\n",
        "print(\"OK model loaded on\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEGmlTPQdmyM",
        "outputId": "247d4d54-4226-4deb-da51-f1417eb9e736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK model loaded on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- helpers ----------\n",
        "def letterbox(img, target=896):\n",
        "    h, w = img.shape[:2]\n",
        "    s = target / max(h, w)\n",
        "    nh, nw = int(round(h * s)), int(round(w * s))\n",
        "    img_r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_CUBIC)\n",
        "    top = (target - nh) // 2; bottom = target - nh - top\n",
        "    left = (target - nw) // 2; right = target - nw - left\n",
        "    img_p = cv2.copyMakeBorder(img_r, top, bottom, left, right,\n",
        "                               cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "    return img_p, (top, bottom, left, right), (h, w)\n",
        "\n",
        "def unletterbox(arr, pads, orig_hw):\n",
        "    top, bottom, left, right = pads\n",
        "    arr = arr[top:arr.shape[0]-bottom, left:arr.shape[1]-right]\n",
        "    return cv2.resize(arr, (orig_hw[1], orig_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_tta_rel(img_bgr):\n",
        "    d0 = model.infer_image(img_bgr).astype(np.float32)\n",
        "    d1 = model.infer_image(cv2.flip(img_bgr, 1)).astype(np.float32)\n",
        "    d1 = np.flip(d1, axis=1)\n",
        "    return (0.5 * (d0 + d1)).astype(np.float32)\n",
        "\n",
        "def to_u16_rel(depth):\n",
        "    d = depth.astype(np.float32)\n",
        "    d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    if (d > 0).sum() < 10:\n",
        "        return np.zeros_like(d, dtype=np.uint16)\n",
        "    p1, p99 = np.percentile(d[d > 0], (1, 99))\n",
        "    d = np.clip(d, np.float32(p1), np.float32(p99))\n",
        "    d = (d - np.float32(p1)) / max(np.float32(1e-6), (np.float32(p99) - np.float32(p1)))\n",
        "    d = np.clip(d, np.float32(1e-6), np.float32(1.0))\n",
        "    return (d * np.float32(65535.0)).astype(np.uint16)"
      ],
      "metadata": {
        "id": "RaVvthCAfL5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- ERD-aligned runner ----------\n",
        "def run_depth_job(image_path, user_id=None, scene_name=\"depth_scene\"):\n",
        "    \"\"\"\n",
        "    ERD-aligned:\n",
        "    - creates vr_scenes\n",
        "    - creates ai_jobs(stage='depth_estimation')\n",
        "    - uploads input + depth outputs to storage\n",
        "    - inserts pipeline_outputs tied to (job_id, vr_scene_id)\n",
        "    \"\"\"\n",
        "\n",
        "    # read image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(\"Failed to read image: \" + image_path)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    # 1) create vr_scene (attach user_id if you have it, else leave null by not passing)\n",
        "    scene_payload = {\"scene_url\": None, \"processing_status\": \"processing\"}\n",
        "    if user_id is not None:\n",
        "        scene_payload[\"user_id\"] = user_id\n",
        "\n",
        "    scene = supabase.table(\"vr_scenes\").insert(scene_payload).execute()\n",
        "    vr_scene_id = scene.data[0][\"id\"]\n",
        "\n",
        "    # 2) create ai_job for depth\n",
        "    job = supabase.table(\"ai_jobs\").insert({\n",
        "        \"vr_scene_id\": vr_scene_id,\n",
        "        \"stage\": \"depth_estimation\",\n",
        "        \"status\": \"running\",\n",
        "        \"model_name\": \"DepthAnythingV2\",\n",
        "        \"model_version\": \"vitl\",\n",
        "        \"params\": {\"tta\": True, \"input_size\": 896},\n",
        "        \"attempt\": 1\n",
        "    }).execute()\n",
        "    job_id = job.data[0][\"id\"]\n",
        "\n",
        "    base = f\"scenes/{vr_scene_id}/jobs/{job_id}\"\n",
        "\n",
        "    try:\n",
        "        # 3) upload input\n",
        "        input_uri = f\"{base}/input/original.jpg\"\n",
        "        upload_bytes(input_uri, encode_jpg(img, 95), \"image/jpeg\")\n",
        "\n",
        "        supabase.table(\"pipeline_outputs\").insert({\n",
        "            \"job_id\": job_id,\n",
        "            \"vr_scene_id\": vr_scene_id,\n",
        "            \"output_type\": \"input_image\",\n",
        "            \"uri\": input_uri,\n",
        "            \"mime_type\": \"image/jpeg\",\n",
        "            \"size_bytes\": None,\n",
        "            \"meta\": {\"width\": w, \"height\": h}\n",
        "        }).execute()\n",
        "\n",
        "        # 4) depth inference\n",
        "        img_p, pads, orig_hw = letterbox(img, 896)\n",
        "        depth_rel_p = infer_tta_rel(img_p)\n",
        "        depth_rel = unletterbox(depth_rel_p, pads, orig_hw).astype(np.float32)\n",
        "\n",
        "        rel16 = to_u16_rel(depth_rel)\n",
        "        viz = cv2.applyColorMap(255 - (rel16 // 256).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
        "\n",
        "        # 5) upload outputs\n",
        "        depth_uri = f\"{base}/depth/depth_rel_u16.png\"\n",
        "        viz_uri   = f\"{base}/depth/depth_viz.jpg\"\n",
        "        upload_bytes(depth_uri, encode_png_u16(rel16), \"image/png\")\n",
        "        upload_bytes(viz_uri, encode_jpg(viz, 92), \"image/jpeg\")\n",
        "\n",
        "        supabase.table(\"pipeline_outputs\").insert({\n",
        "            \"job_id\": job_id,\n",
        "            \"vr_scene_id\": vr_scene_id,\n",
        "            \"output_type\": \"depth\",\n",
        "            \"uri\": depth_uri,\n",
        "            \"mime_type\": \"image/png\",\n",
        "            \"size_bytes\": None,\n",
        "            \"meta\": {\"encoding\": \"u16_rel\", \"tta\": True, \"input_size\": 896}\n",
        "        }).execute()\n",
        "\n",
        "        supabase.table(\"pipeline_outputs\").insert({\n",
        "            \"job_id\": job_id,\n",
        "            \"vr_scene_id\": vr_scene_id,\n",
        "            \"output_type\": \"preview\",\n",
        "            \"uri\": viz_uri,\n",
        "            \"mime_type\": \"image/jpeg\",\n",
        "            \"size_bytes\": None,\n",
        "            \"meta\": {\"kind\": \"depth_viz\", \"colormap\": \"inferno\"}\n",
        "        }).execute()\n",
        "\n",
        "        # 6) mark job + scene success\n",
        "        supabase.table(\"ai_jobs\").update({\n",
        "            \"status\": \"succeeded\",\n",
        "            \"finished_at\": \"now()\"\n",
        "        }).eq(\"id\", job_id).execute()\n",
        "\n",
        "        supabase.table(\"vr_scenes\").update({\n",
        "            \"processing_status\": \"succeeded\"\n",
        "        }).eq(\"id\", vr_scene_id).execute()\n",
        "\n",
        "        return {\"vr_scene_id\": vr_scene_id, \"job_id\": job_id, \"input\": input_uri, \"depth\": depth_uri, \"viz\": viz_uri}\n",
        "\n",
        "    except Exception as e:\n",
        "        supabase.table(\"ai_jobs\").update({\n",
        "            \"status\": \"failed\",\n",
        "            \"error\": str(e)\n",
        "        }).eq(\"id\", job_id).execute()\n",
        "\n",
        "        supabase.table(\"vr_scenes\").update({\n",
        "            \"processing_status\": \"failed\"\n",
        "        }).eq(\"id\", vr_scene_id).execute()\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "pfmzaKNsdCpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = run_depth_job(\"/content/drive/MyDrive/Segmentation/dataset/157_00150.png\")\n",
        "out\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS4jxpcdfrhv",
        "outputId": "5ea32483-c874-4607-d5a8-1dcdac9ce8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vr_scene_id': '1b7d7887-85da-49f9-bb73-a435d1cc1620',\n",
              " 'job_id': '80d36f06-2834-40ea-9d84-4ccc2fa5c2ea',\n",
              " 'input': 'scenes/1b7d7887-85da-49f9-bb73-a435d1cc1620/jobs/80d36f06-2834-40ea-9d84-4ccc2fa5c2ea/input/original.jpg',\n",
              " 'depth': 'scenes/1b7d7887-85da-49f9-bb73-a435d1cc1620/jobs/80d36f06-2834-40ea-9d84-4ccc2fa5c2ea/depth/depth_rel_u16.png',\n",
              " 'viz': 'scenes/1b7d7887-85da-49f9-bb73-a435d1cc1620/jobs/80d36f06-2834-40ea-9d84-4ccc2fa5c2ea/depth/depth_viz.jpg'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation"
      ],
      "metadata": {
        "id": "1kfr9GHvAtqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, cv2, numpy as np, torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "BUCKET = \"pipeline\"  # keep consistent\n",
        "\n",
        "# ---- choose model ----\n",
        "MODEL_DIR = \"/content/drive/MyDrive/FullScene_SegPipeline_V2/best_student_b5\"  # <-- change to yours\n",
        "# MODEL_DIR = \"nvidia/segformer-b5-finetuned-ade-640-640\"               # alt\n",
        "\n",
        "# ---- your 13 classes (must match training if using best_student) ----\n",
        "SUPER = [\n",
        "    \"background\",\"person\",\"floor\",\"wall\",\"ceiling\",\"road/sidewalk\",\"building\",\n",
        "    \"furniture\",\"window/door\",\"vegetation\",\"sky\",\"water\",\"other\"\n",
        "]\n",
        "K = len(SUPER)\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(MODEL_DIR)\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(MODEL_DIR).to(device).eval()\n",
        "print(\"OK model loaded:\", MODEL_DIR)\n",
        "\n",
        "def upload_bytes(path, content_bytes, content_type):\n",
        "    supabase.storage.from_(BUCKET).upload(\n",
        "        path=path,\n",
        "        file=content_bytes,\n",
        "        file_options={\"content-type\": content_type, \"upsert\": \"true\"}\n",
        "    )\n",
        "\n",
        "def encode_png_u8(u8):\n",
        "    ok, buf = cv2.imencode(\".png\", u8)\n",
        "    if not ok: raise RuntimeError(\"PNG encode failed\")\n",
        "    return buf.tobytes()\n",
        "\n",
        "def encode_jpg(bgr, quality=92):\n",
        "    ok, buf = cv2.imencode(\".jpg\", bgr, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
        "    if not ok: raise RuntimeError(\"JPG encode failed\")\n",
        "    return buf.tobytes()\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_seg_class_ids(pil_img: Image.Image):\n",
        "    \"\"\"Return (H,W) uint8 class-id mask in [0..K-1] at original resolution.\"\"\"\n",
        "    orig_w, orig_h = pil_img.size\n",
        "    inputs = processor(images=pil_img, return_tensors=\"pt\")\n",
        "    pv = inputs[\"pixel_values\"].to(device)\n",
        "\n",
        "    with torch.amp.autocast(\"cuda\", enabled=(device == \"cuda\")):\n",
        "        logits = model(pixel_values=pv).logits  # (1,K,h,w)\n",
        "\n",
        "    logits_up = F.interpolate(logits, size=(orig_h, orig_w), mode=\"bilinear\", align_corners=False)\n",
        "    pred = torch.argmax(logits_up, dim=1)[0].detach().cpu().numpy().astype(np.uint8)\n",
        "    return pred\n",
        "\n",
        "def render_overlay(rgb, mask_ids, alpha=0.35):\n",
        "    # simple deterministic palette for debug (not a “contract”, just visualization)\n",
        "    rng = np.random.default_rng(0)\n",
        "    palette = rng.integers(0, 255, size=(K, 3), dtype=np.uint8)\n",
        "    color = palette[mask_ids]  # (H,W,3)\n",
        "    over = (rgb.astype(np.float32)*(1-alpha) + color.astype(np.float32)*alpha).astype(np.uint8)\n",
        "    return over, palette\n",
        "\n",
        "def run_seg_job(vr_scene_id, input_image_uri):\n",
        "    \"\"\"\n",
        "    ERD-aligned segmentation job:\n",
        "    - creates ai_jobs(stage='segmentation')\n",
        "    - downloads input from storage\n",
        "    - runs model\n",
        "    - uploads seg outputs\n",
        "    - inserts pipeline_outputs\n",
        "    \"\"\"\n",
        "    # 1) create ai_job\n",
        "    job = supabase.table(\"ai_jobs\").insert({\n",
        "        \"vr_scene_id\": vr_scene_id,\n",
        "        \"stage\": \"segmentation\",\n",
        "        \"status\": \"running\",\n",
        "        \"model_name\": \"SegFormer\",\n",
        "        \"model_version\": str(MODEL_DIR),\n",
        "        \"params\": {\"classes\": K},\n",
        "        \"attempt\": 1\n",
        "    }).execute()\n",
        "    job_id = job.data[0][\"id\"]\n",
        "    base = f\"scenes/{vr_scene_id}/jobs/{job_id}\"\n",
        "\n",
        "    try:\n",
        "        # 2) download input from storage\n",
        "        blob = supabase.storage.from_(BUCKET).download(input_image_uri)\n",
        "        pil = Image.open(io.BytesIO(blob)).convert(\"RGB\")\n",
        "        rgb = np.array(pil)  # RGB uint8\n",
        "\n",
        "        # 3) infer\n",
        "        mask_ids = infer_seg_class_ids(pil)  # (H,W) uint8 0..K-1\n",
        "\n",
        "        # 4) overlay for debug\n",
        "        over_rgb, palette = render_overlay(rgb, mask_ids, alpha=0.35)\n",
        "        over_bgr = cv2.cvtColor(over_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # 5) upload outputs\n",
        "        mask_uri = f\"{base}/seg/seg_mask_id.png\"\n",
        "        ov_uri   = f\"{base}/seg/seg_overlay.jpg\"\n",
        "\n",
        "        upload_bytes(mask_uri, encode_png_u8(mask_ids), \"image/png\")\n",
        "        upload_bytes(ov_uri, encode_jpg(over_bgr, 92), \"image/jpeg\")\n",
        "\n",
        "        # 6) pipeline_outputs rows\n",
        "        supabase.table(\"pipeline_outputs\").insert({\n",
        "            \"job_id\": job_id,\n",
        "            \"vr_scene_id\": vr_scene_id,\n",
        "            \"output_type\": \"mask\",\n",
        "            \"uri\": mask_uri,\n",
        "            \"mime_type\": \"image/png\",\n",
        "            \"meta\": {\"format\": \"class_id_u8\", \"classes\": SUPER}\n",
        "        }).execute()\n",
        "\n",
        "        supabase.table(\"pipeline_outputs\").insert({\n",
        "            \"job_id\": job_id,\n",
        "            \"vr_scene_id\": vr_scene_id,\n",
        "            \"output_type\": \"preview\",\n",
        "            \"uri\": ov_uri,\n",
        "            \"mime_type\": \"image/jpeg\",\n",
        "            \"meta\": {\"kind\": \"seg_overlay\"}\n",
        "        }).execute()\n",
        "\n",
        "        # 7) mark job done\n",
        "        supabase.table(\"ai_jobs\").update({\"status\": \"succeeded\"}).eq(\"id\", job_id).execute()\n",
        "\n",
        "        return {\"job_id\": job_id, \"mask\": mask_uri, \"overlay\": ov_uri}\n",
        "\n",
        "    except Exception as e:\n",
        "        supabase.table(\"ai_jobs\").update({\"status\": \"failed\", \"error\": str(e)}).eq(\"id\", job_id).execute()\n",
        "        raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "3994fd4714174bfba8c9aec71aa61faf",
            "50d38cc72e434e779ae920108b78e0f6",
            "e4df76fe7c004a33b6fb6debdb4a19ef",
            "3d5ee9904e3e4974aa2a3bd80e3ccaf6",
            "e5012bf89e9c484f86deefde708b24bd",
            "802db505ebff4e0ba803717d9efc11db",
            "494ca2d0cbaa4bb09a01d601de9e6dc8",
            "24cf0c650fec47dd9e69e5c4edf8c7be",
            "d0438655e6e24077acf8db0e0421c8c2",
            "f90f95c6652d4f51bb47ae8a9e8b6c06",
            "faaafed6b947440c80dc5cdb0a63590d"
          ]
        },
        "id": "DRFLxABAA3N1",
        "outputId": "12743fad-ee8e-49ab-c107-fb44cddbc76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/1172 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3994fd4714174bfba8c9aec71aa61faf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK model loaded: /content/drive/MyDrive/FullScene_SegPipeline_V2/best_student_b5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vr_scene_id = \"ffb2919e-6418-42ab-94ba-39757ed82d66\"\n",
        "\n",
        "res = supabase.table(\"pipeline_outputs\") \\\n",
        "    .select(\"job_id, output_type, uri, created_at\") \\\n",
        "    .eq(\"vr_scene_id\", vr_scene_id) \\\n",
        "    .eq(\"output_type\", \"input_image\") \\\n",
        "    .order(\"created_at\", desc=True) \\\n",
        "    .limit(1) \\\n",
        "    .execute()\n",
        "\n",
        "print(res.data)\n",
        "input_uri = res.data[0][\"uri\"]\n",
        "print(\"INPUT_URI =\", input_uri)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQLGLtCtB2N6",
        "outputId": "b3f03d8d-617f-48c2-e1e1-6784df0c7aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'job_id': '29b79e22-5d48-44c2-9998-fcb4f7d677fa', 'output_type': 'input_image', 'uri': 'scenes/ffb2919e-6418-42ab-94ba-39757ed82d66/jobs/29b79e22-5d48-44c2-9998-fcb4f7d677fa/input/original.jpg', 'created_at': '2026-02-01T22:35:39.136514+00:00'}]\n",
            "INPUT_URI = scenes/ffb2919e-6418-42ab-94ba-39757ed82d66/jobs/29b79e22-5d48-44c2-9998-fcb4f7d677fa/input/original.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io, cv2, numpy as np, torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, SegformerForSemanticSegmentation\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BUCKET = \"pipeline\"  # must match your bucket exactly\n",
        "\n",
        "# ✅ set this to your local model folder\n",
        "MODEL_DIR = \"/content/drive/MyDrive/FullScene_SegPipeline_V2/best_student_b5\"\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(MODEL_DIR)\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(MODEL_DIR).to(device).eval()\n",
        "print(\"OK seg model loaded from:\", MODEL_DIR)\n",
        "\n",
        "def upload_bytes(path, content_bytes, content_type):\n",
        "    supabase.storage.from_(BUCKET).upload(\n",
        "        path=path,\n",
        "        file=content_bytes,\n",
        "        file_options={\"content-type\": content_type, \"upsert\": \"true\"}\n",
        "    )\n",
        "\n",
        "def encode_png_u8(u8):\n",
        "    ok, buf = cv2.imencode(\".png\", u8)\n",
        "    if not ok: raise RuntimeError(\"PNG encode failed\")\n",
        "    return buf.tobytes()\n",
        "\n",
        "def encode_jpg(bgr, quality=92):\n",
        "    ok, buf = cv2.imencode(\".jpg\", bgr, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
        "    if not ok: raise RuntimeError(\"JPG encode failed\")\n",
        "    return buf.tobytes()\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_seg_ids(pil_img: Image.Image):\n",
        "    orig_w, orig_h = pil_img.size\n",
        "    inputs = processor(images=pil_img, return_tensors=\"pt\")\n",
        "    pv = inputs[\"pixel_values\"].to(device)\n",
        "\n",
        "    with torch.amp.autocast(\"cuda\", enabled=(device == \"cuda\")):\n",
        "        logits = model(pixel_values=pv).logits  # (1,K,h,w)\n",
        "\n",
        "    logits_up = F.interpolate(logits, size=(orig_h, orig_w), mode=\"bilinear\", align_corners=False)\n",
        "    pred = torch.argmax(logits_up, dim=1)[0].detach().cpu().numpy().astype(np.uint8)\n",
        "    return pred\n",
        "\n",
        "def render_overlay(rgb, mask_ids, alpha=0.35, seed=0):\n",
        "    K = int(mask_ids.max()) + 1 if mask_ids.size else 1\n",
        "    rng = np.random.default_rng(seed)\n",
        "    palette = rng.integers(0, 255, size=(max(K, 16), 3), dtype=np.uint8)\n",
        "    color = palette[mask_ids]\n",
        "    over = (rgb.astype(np.float32)*(1-alpha) + color.astype(np.float32)*alpha).astype(np.uint8)\n",
        "    return over\n",
        "\n",
        "def run_seg_job(vr_scene_id, input_uri):\n",
        "    # 1) create segmentation job\n",
        "    job = supabase.table(\"ai_jobs\").insert({\n",
        "        \"vr_scene_id\": vr_scene_id,\n",
        "        \"stage\": \"segmentation\",\n",
        "        \"status\": \"running\",\n",
        "        \"model_name\": \"SegFormer\",\n",
        "        \"model_version\": \"local_best_student\",\n",
        "        \"params\": {\"model_dir\": MODEL_DIR},\n",
        "        \"attempt\": 1\n",
        "    }).execute()\n",
        "    job_id = job.data[0][\"id\"]\n",
        "    base = f\"scenes/{vr_scene_id}/jobs/{job_id}\"\n",
        "\n",
        "    try:\n",
        "        # 2) download input image from storage\n",
        "        blob = supabase.storage.from_(BUCKET).download(input_uri)\n",
        "        pil = Image.open(io.BytesIO(blob)).convert(\"RGB\")\n",
        "        rgb = np.array(pil)  # RGB\n",
        "\n",
        "        # 3) infer\n",
        "        mask_ids = infer_seg_ids(pil)\n",
        "\n",
        "        # 4) overlay for debug\n",
        "        over_rgb = render_overlay(rgb, mask_ids, alpha=0.35)\n",
        "        over_bgr = cv2.cvtColor(over_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # 5) upload outputs\n",
        "        mask_uri = f\"{base}/seg/seg_mask_id.png\"\n",
        "        ov_uri   = f\"{base}/seg/seg_overlay.jpg\"\n",
        "\n",
        "        upload_bytes(mask_uri, encode_png_u8(mask_ids), \"image/png\")\n",
        "        upload_bytes(ov_uri, encode_jpg(over_bgr, 92), \"image/jpeg\")\n",
        "\n",
        "        # 6) DB rows\n",
        "        supabase.table(\"pipeline_outputs\").insert({\n",
        "            \"job_id\": job_id,\n",
        "            \"vr_scene_id\": vr_scene_id,\n",
        "            \"output_type\": \"mask\",\n",
        "            \"uri\": mask_uri,\n",
        "            \"mime_type\": \"image/png\",\n",
        "            \"meta\": {\"format\": \"class_id_u8\"}\n",
        "        }).execute()\n",
        "\n",
        "        supabase.table(\"pipeline_outputs\").insert({\n",
        "            \"job_id\": job_id,\n",
        "            \"vr_scene_id\": vr_scene_id,\n",
        "            \"output_type\": \"preview\",\n",
        "            \"uri\": ov_uri,\n",
        "            \"mime_type\": \"image/jpeg\",\n",
        "            \"meta\": {\"kind\": \"seg_overlay\"}\n",
        "        }).execute()\n",
        "\n",
        "        supabase.table(\"ai_jobs\").update({\"status\": \"succeeded\"}).eq(\"id\", job_id).execute()\n",
        "\n",
        "        return {\"job_id\": job_id, \"mask_uri\": mask_uri, \"overlay_uri\": ov_uri}\n",
        "\n",
        "    except Exception as e:\n",
        "        supabase.table(\"ai_jobs\").update({\"status\": \"failed\", \"error\": str(e)}).eq(\"id\", job_id).execute()\n",
        "        raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "4cb892c983aa4bc2a9a1423d42dc62a4",
            "8e12294c0375460dbf13b69337c9f08e",
            "d3e72944ba4844aaa11d85d4e548445a",
            "525bcf56d14b4f22b8b41195b71a40e9",
            "8c46f24cb3bb40c9949c1c389d2f6e9a",
            "d50e9bb7e56a402f91e849149259fc2e",
            "f89398d0d5b7486aa99687870097fe84",
            "03960145c83143ba995d65ca4233d9d3",
            "c5e41e1c8ad44c0a8e1df8b312f1b66a",
            "f053d54dc8b7482a8c345985ff62eb12",
            "230f5266ebd041c58caae879f7bee096"
          ]
        },
        "id": "WRYyoBZTClzS",
        "outputId": "5b77fd91-1bab-48ec-9a31-25d204749e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/1172 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cb892c983aa4bc2a9a1423d42dc62a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK seg model loaded from: /content/drive/MyDrive/FullScene_SegPipeline_V2/best_student_b5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seg_out = run_seg_job(vr_scene_id, input_uri)\n",
        "seg_out\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aimI3NgBCuwQ",
        "outputId": "fe5569d6-4d5a-46e6-d41a-5f912d563cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'job_id': '0c7c40e7-3ada-49d0-8bc1-86e0cedd9dc2',\n",
              " 'mask_uri': 'scenes/ffb2919e-6418-42ab-94ba-39757ed82d66/jobs/0c7c40e7-3ada-49d0-8bc1-86e0cedd9dc2/seg/seg_mask_id.png',\n",
              " 'overlay_uri': 'scenes/ffb2919e-6418-42ab-94ba-39757ed82d66/jobs/0c7c40e7-3ada-49d0-8bc1-86e0cedd9dc2/seg/seg_overlay.jpg'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying segments"
      ],
      "metadata": {
        "id": "xgXf1TnKXZPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, io, json, cv2, numpy as np\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "BUCKET = \"pipeline\"  # keep consistent\n",
        "\n",
        "# ------------------------------\n",
        "# Helpers\n",
        "# ------------------------------\n",
        "def upload_bytes(path, content_bytes, content_type):\n",
        "    supabase.storage.from_(BUCKET).upload(\n",
        "        path=path,\n",
        "        file=content_bytes,\n",
        "        file_options={\n",
        "            \"content-type\": str(content_type),\n",
        "            \"upsert\": \"true\"   # <-- string, not bool\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def encode_png_bgra(bgra):\n",
        "    ok, buf = cv2.imencode(\".png\", bgra)\n",
        "    if not ok:\n",
        "        raise RuntimeError(\"PNG encode failed\")\n",
        "    return buf.tobytes()\n",
        "\n",
        "def encode_json(obj):\n",
        "    return json.dumps(obj, ensure_ascii=False).encode(\"utf-8\")\n",
        "\n",
        "def clean_binary(binary, k=5, open_it=1, close_it=2):\n",
        "    kernel = np.ones((k, k), np.uint8)\n",
        "    if open_it > 0:\n",
        "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=open_it)\n",
        "    if close_it > 0:\n",
        "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=close_it)\n",
        "    return binary\n",
        "\n",
        "def extract_instances_from_semid(original_bgr, sem_id, class_id, min_area=1200, pad=5):\n",
        "    \"\"\"\n",
        "    Returns list of dicts:\n",
        "    [{\"bbox\":[x0,y0,x1,y1], \"area\":int, \"bgra\":np.ndarray}, ...]\n",
        "    \"\"\"\n",
        "    binary = (sem_id == class_id).astype(np.uint8) * 255\n",
        "    binary = clean_binary(binary, k=5, open_it=1, close_it=2)\n",
        "\n",
        "    n, labels = cv2.connectedComponents(binary)\n",
        "    inst_list = []\n",
        "\n",
        "    for lab in range(1, n):\n",
        "        inst = (labels == lab).astype(np.uint8) * 255\n",
        "        area = int(cv2.countNonZero(inst))\n",
        "        if area < min_area:\n",
        "            continue\n",
        "\n",
        "        x, y, w, h = cv2.boundingRect(inst)\n",
        "\n",
        "        x0 = max(0, x - pad); y0 = max(0, y - pad)\n",
        "        x1 = min(original_bgr.shape[1], x + w + pad)\n",
        "        y1 = min(original_bgr.shape[0], y + h + pad)\n",
        "\n",
        "        crop_rgb  = original_bgr[y0:y1, x0:x1]\n",
        "        crop_mask = inst[y0:y1, x0:x1]\n",
        "\n",
        "        bgra = cv2.cvtColor(crop_rgb, cv2.COLOR_BGR2BGRA)\n",
        "        bgra[:, :, 3] = crop_mask\n",
        "\n",
        "        inst_list.append({\n",
        "            \"bbox\": [int(x0), int(y0), int(x1), int(y1)],\n",
        "            \"area\": area,\n",
        "            \"bgra\": bgra\n",
        "        })\n",
        "\n",
        "    return inst_list\n",
        "\n",
        "def get_latest_output_uri(vr_scene_id, output_type):\n",
        "    res = supabase.table(\"pipeline_outputs\") \\\n",
        "        .select(\"uri, created_at\") \\\n",
        "        .eq(\"vr_scene_id\", vr_scene_id) \\\n",
        "        .eq(\"output_type\", output_type) \\\n",
        "        .order(\"created_at\", desc=True) \\\n",
        "        .limit(1) \\\n",
        "        .execute()\n",
        "    if not res.data:\n",
        "        raise ValueError(f\"No pipeline_outputs found for vr_scene_id={vr_scene_id}, output_type={output_type}\")\n",
        "    return res.data[0][\"uri\"]\n",
        "\n",
        "# ------------------------------\n",
        "# Main integrated stage\n",
        "# ------------------------------\n",
        "def run_instance_extraction_job(\n",
        "    vr_scene_id,\n",
        "    targets,                 # dict: {\"person\":1, \"object\":13, ...}\n",
        "    min_area=1200,\n",
        "    pad=5\n",
        "):\n",
        "    \"\"\"\n",
        "    ERD-aligned:\n",
        "    - creates ai_jobs(stage='classification')\n",
        "    - downloads input_image + mask from storage\n",
        "    - extracts per-instance BGRA crops for each target class\n",
        "    - uploads crops\n",
        "    - inserts pipeline_outputs rows for each crop + a manifest JSON\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Create job\n",
        "    job = supabase.table(\"ai_jobs\").insert({\n",
        "        \"vr_scene_id\": vr_scene_id,\n",
        "        \"stage\": \"classification\",\n",
        "        \"status\": \"running\",\n",
        "        \"model_name\": \"connectedComponents\",\n",
        "        \"model_version\": \"opencv\",\n",
        "        \"params\": {\"min_area\": min_area, \"pad\": pad, \"targets\": targets},\n",
        "        \"attempt\": 1\n",
        "    }).execute()\n",
        "\n",
        "    job_id = job.data[0][\"id\"]\n",
        "    base = f\"scenes/{vr_scene_id}/jobs/{job_id}\"\n",
        "\n",
        "    try:\n",
        "        # 2) Pull latest input + mask URIs\n",
        "        input_uri = get_latest_output_uri(vr_scene_id, \"input_image\")\n",
        "        mask_uri  = get_latest_output_uri(vr_scene_id, \"mask\")\n",
        "\n",
        "        # 3) Download input image + mask\n",
        "        input_blob = supabase.storage.from_(BUCKET).download(input_uri)\n",
        "        mask_blob  = supabase.storage.from_(BUCKET).download(mask_uri)\n",
        "\n",
        "        input_arr = np.frombuffer(input_blob, dtype=np.uint8)\n",
        "        original = cv2.imdecode(input_arr, cv2.IMREAD_COLOR)\n",
        "        if original is None:\n",
        "            raise ValueError(\"Failed to decode input image from storage: \" + input_uri)\n",
        "\n",
        "        mask_arr = np.frombuffer(mask_blob, dtype=np.uint8)\n",
        "        sem_id = cv2.imdecode(mask_arr, cv2.IMREAD_UNCHANGED)\n",
        "        if sem_id is None:\n",
        "            raise ValueError(\"Failed to decode mask from storage: \" + mask_uri)\n",
        "\n",
        "        if sem_id.ndim == 3:\n",
        "            sem_id = sem_id[:, :, 0]\n",
        "\n",
        "        if original.shape[:2] != sem_id.shape[:2]:\n",
        "            sem_id = cv2.resize(sem_id, (original.shape[1], original.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        H, W = original.shape[:2]\n",
        "\n",
        "        # 4) Extract + upload\n",
        "        manifest = {\n",
        "            \"vr_scene_id\": vr_scene_id,\n",
        "            \"job_id\": job_id,\n",
        "            \"input_uri\": input_uri,\n",
        "            \"mask_uri\": mask_uri,\n",
        "            \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
        "            \"image_hw\": [int(H), int(W)],\n",
        "            \"targets\": targets,\n",
        "            \"instances\": []\n",
        "        }\n",
        "\n",
        "        total = 0\n",
        "        for class_name, class_id in targets.items():\n",
        "            inst_list = extract_instances_from_semid(\n",
        "                original_bgr=original,\n",
        "                sem_id=sem_id,\n",
        "                class_id=int(class_id),\n",
        "                min_area=min_area,\n",
        "                pad=pad\n",
        "            )\n",
        "\n",
        "            for i, inst in enumerate(inst_list):\n",
        "                out_uri = f\"{base}/instances/{class_name}/{class_name}_{i:03d}.png\"\n",
        "                png_bytes = encode_png_bgra(inst[\"bgra\"])\n",
        "\n",
        "                upload_bytes(out_uri, png_bytes, \"image/png\")\n",
        "\n",
        "                # pipeline_outputs row per crop\n",
        "                supabase.table(\"pipeline_outputs\").insert({\n",
        "                    \"job_id\": job_id,\n",
        "                    \"vr_scene_id\": vr_scene_id,\n",
        "                    \"output_type\": \"instance_crop\",\n",
        "                    \"uri\": out_uri,\n",
        "                    \"mime_type\": \"image/png\",\n",
        "                    \"size_bytes\": len(png_bytes),\n",
        "                    \"meta\": {\n",
        "                        \"class_name\": class_name,\n",
        "                        \"class_id\": int(class_id),\n",
        "                        \"bbox\": inst[\"bbox\"],\n",
        "                        \"area\": inst[\"area\"]\n",
        "                    }\n",
        "                }).execute()\n",
        "\n",
        "                manifest[\"instances\"].append({\n",
        "                    \"class_name\": class_name,\n",
        "                    \"class_id\": int(class_id),\n",
        "                    \"uri\": out_uri,\n",
        "                    \"bbox\": inst[\"bbox\"],\n",
        "                    \"area\": inst[\"area\"]\n",
        "                })\n",
        "                total += 1\n",
        "\n",
        "        # 5) Upload manifest (optional but HIGHLY recommended)\n",
        "        manifest_uri = f\"{base}/instances/manifest.json\"\n",
        "        man_bytes = encode_json(manifest)\n",
        "        upload_bytes(manifest_uri, man_bytes, \"application/json\")\n",
        "\n",
        "        supabase.table(\"pipeline_outputs\").insert({\n",
        "            \"job_id\": job_id,\n",
        "            \"vr_scene_id\": vr_scene_id,\n",
        "            \"output_type\": \"manifest\",\n",
        "            \"uri\": manifest_uri,\n",
        "            \"mime_type\": \"application/json\",\n",
        "            \"size_bytes\": len(man_bytes),\n",
        "            \"meta\": {\"kind\": \"instance_manifest\", \"count\": total}\n",
        "        }).execute()\n",
        "\n",
        "        # 6) Mark job success\n",
        "        supabase.table(\"ai_jobs\").update({\n",
        "            \"status\": \"succeeded\"\n",
        "        }).eq(\"id\", job_id).execute()\n",
        "\n",
        "        return {\"job_id\": job_id, \"count\": total, \"manifest_uri\": manifest_uri}\n",
        "\n",
        "    except Exception as e:\n",
        "        supabase.table(\"ai_jobs\").update({\n",
        "            \"status\": \"failed\",\n",
        "            \"error\": str(e)\n",
        "        }).eq(\"id\", job_id).execute()\n",
        "        raise\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cPM7VT0NXjt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upload_bytes(\"debug/test.txt\", b\"hello\", \"text/plain\")\n",
        "print(\"upload ok\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWRJeXHbfFzc",
        "outputId": "dc44c767-01cb-4c2d-a8b1-d4c1e08720de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upload ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "targets = {\n",
        "    \"person\": 1,\n",
        "    \"object\": 13\n",
        "}\n",
        "\n",
        "out = run_instance_extraction_job(\n",
        "    vr_scene_id=\"ffb2919e-6418-42ab-94ba-39757ed82d66\",\n",
        "    targets=targets,\n",
        "    min_area=1200,\n",
        "    pad=5\n",
        ")\n",
        "out\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVkfAb8AXwuJ",
        "outputId": "4b7c71d3-5064-46fb-cfb9-c018ff68726c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'job_id': '907abc88-1bda-4abd-b7fe-5a03e0c387b4',\n",
              " 'count': 1,\n",
              " 'manifest_uri': 'scenes/ffb2919e-6418-42ab-94ba-39757ed82d66/jobs/907abc88-1bda-4abd-b7fe-5a03e0c387b4/instances/manifest.json'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}