{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9565f9f2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Code (vits model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, sys, cv2, numpy as np, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "IMG_DIR  = \"/content/drive/MyDrive/Depthanythingv2/data/eval\"\n",
    "REPO     = \"/content/Depth-Anything-V2\"\n",
    "CKPT     = f\"{REPO}/checkpoints/depth_anything_v2_vits.pth\"  # vits checkpoint\n",
    "OUT_REL  = \"/content/drive/MyDrive/Depthanythingv2/output-vits/depths/raw_da2_vits\"\n",
    "OUT_VIZ  = \"/content/drive/MyDrive/Depthanythingv2/output-vits/depths/viz_da2_vits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "\n",
    "encoder = 'vits' # or 'vits', 'vitb', 'vitg'\n",
    "\n",
    "model = DepthAnythingV2(**model_configs[encoder])\n",
    "model.load_state_dict(torch.load(f'/content/drive/MyDrive/Depthanythingv2/checkpoints/depth_anything_v2_{encoder}.pth', map_location='cpu'))\n",
    "model = model.to(DEVICE).eval()\n",
    "\n",
    " # HxW raw depth map in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8b02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:59<00:00,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 18 relative depth maps to /content/drive/MyDrive/Depthanythingv2/output-vits/depths/raw_da2_vits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def to_u16_rel(depth):\n",
    "    \"\"\"Normalize relative depth to 16-bit safely (no exact zeros).\"\"\"\n",
    "    d = depth.astype(np.float32)\n",
    "    d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if (d > 0).sum() < 10:\n",
    "        return np.zeros_like(d, dtype=np.uint16)\n",
    "    lo, hi = np.percentile(d[d > 0], (1, 99))\n",
    "    d = np.clip(d, lo, hi)\n",
    "    d = (d - lo) / max(1e-6, (hi - lo))\n",
    "    d = np.clip(d, 1e-6, 1.0)  # avoid exact zeros\n",
    "    return (d * 65535.0).astype(np.uint16)\n",
    "\n",
    "def letterbox(img, target=768):\n",
    "    h, w = img.shape[:2]\n",
    "    s = target / max(h, w)\n",
    "    nh, nw = int(round(h*s)), int(round(w*s))\n",
    "    img_r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_CUBIC)\n",
    "    top = (target - nh)//2; bottom = target - nh - top\n",
    "    left = (target - nw)//2; right = target - nw - left\n",
    "    img_p = cv2.copyMakeBorder(img_r, top,bottom,left,right, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "    return img_p, (top,bottom,left,right), (h,w)\n",
    "\n",
    "def unletterbox(arr, pads, orig_hw):\n",
    "    top,bottom,left,right = pads\n",
    "    arr = arr[top:arr.shape[0]-bottom, left:arr.shape[1]-right]\n",
    "    return cv2.resize(arr, (orig_hw[1], orig_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# --- Collect images ---\n",
    "paths = sorted(p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
    "               for p in glob.glob(os.path.join(IMG_DIR, ext)))\n",
    "\n",
    "# --- Inference loop (prefer 768 input; fallback if not supported) ---\n",
    "import inspect\n",
    "sig = str(getattr(model, \"infer_image\", None))\n",
    "supports_size = \"input_size\" in (sig or \"\")\n",
    "\n",
    "for p in tqdm(paths):\n",
    "    img = cv2.imread(p)\n",
    "\n",
    "    if supports_size:\n",
    "        depth_rel = model.infer_image(img, input_size=768)  # HxW float (relative)\n",
    "    else:\n",
    "        # fallback: letterbox to 768, run, then unpad\n",
    "        img_p, pads, orig_hw = letterbox(img, 768)\n",
    "        depth_rel_p = model.infer_image(img_p)\n",
    "        depth_rel   = unletterbox(depth_rel_p, pads, orig_hw).astype(np.float32)\n",
    "\n",
    "    rel16 = to_u16_rel(depth_rel)\n",
    "    stem = os.path.splitext(os.path.basename(p))[0]\n",
    "    cv2.imwrite(f\"{OUT_REL}/{stem}.png\", rel16)\n",
    "    viz = cv2.applyColorMap(255 - (rel16 // 256).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
    "    cv2.imwrite(f\"{OUT_VIZ}/{stem}.jpg\", viz)\n",
    "\n",
    "print(\"Saved:\", len(paths), \"relative depth maps to\", OUT_REL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa84ec6",
   "metadata": {},
   "source": [
    "### Evaluation (vits model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e7fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:06<00:00,  2.74it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"img_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"c356067f-132c-4032-a812-4d8a405f6a92\",\n          \"de1ad16a-c56d-45ed-ae2a-35da38c4f0aa\",\n          \"e14c21ec-33d6-4aee-b254-9349418d5008\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edge_prec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05279695820391032,\n        \"min\": 0.2815376809139728,\n        \"max\": 0.3930043088571661,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.289000552160103,\n          0.2815376809139728,\n          0.3930043088571661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edge_rec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05278987029054298,\n        \"min\": 0.2815376809139728,\n        \"max\": 0.39300468242901454,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.289000552160103,\n          0.2815376809139728,\n          0.39300468242901454\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edge_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05279341295379118,\n        \"min\": 0.2815376809139728,\n        \"max\": 0.3930044956430015,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.289000552160103,\n          0.2815376809139728,\n          0.3930044956430015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ordinal_fg_bg_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"planarity_residual_rel\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03607526729833916,\n        \"min\": 0.13993470394266697,\n        \"max\": 0.2189671892023535,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"invalid_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007397216234813724,\n        \"min\": 0.010058504746880526,\n        \"max\": 0.025765044071840437,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"overall_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.035310000239907824,\n        \"min\": 0.3110849842219082,\n        \"max\": 0.38708478876343955,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-73e414f1-accd-4b6c-a604-2c2f550227f5\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>edge_prec</th>\n",
       "      <th>edge_rec</th>\n",
       "      <th>edge_f1</th>\n",
       "      <th>ordinal_fg_bg_acc</th>\n",
       "      <th>planarity_residual_rel</th>\n",
       "      <th>invalid_ratio</th>\n",
       "      <th>overall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e14c21ec-33d6-4aee-b254-9349418d5008</td>\n",
       "      <td>0.393004</td>\n",
       "      <td>0.393005</td>\n",
       "      <td>0.393004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.218967</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>0.387085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c356067f-132c-4032-a812-4d8a405f6a92</td>\n",
       "      <td>0.289001</td>\n",
       "      <td>0.289001</td>\n",
       "      <td>0.289001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.151955</td>\n",
       "      <td>0.019939</td>\n",
       "      <td>0.325722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>877eef99-d724-4928-93ca-8abfb07cf90a</td>\n",
       "      <td>0.292972</td>\n",
       "      <td>0.293027</td>\n",
       "      <td>0.292999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.139935</td>\n",
       "      <td>0.025712</td>\n",
       "      <td>0.315776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de1ad16a-c56d-45ed-ae2a-35da38c4f0aa</td>\n",
       "      <td>0.281538</td>\n",
       "      <td>0.281538</td>\n",
       "      <td>0.281538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.151061</td>\n",
       "      <td>0.025765</td>\n",
       "      <td>0.311085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73e414f1-accd-4b6c-a604-2c2f550227f5')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-73e414f1-accd-4b6c-a604-2c2f550227f5 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-73e414f1-accd-4b6c-a604-2c2f550227f5');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                 img_id  edge_prec  edge_rec   edge_f1  \\\n",
       "3  e14c21ec-33d6-4aee-b254-9349418d5008   0.393004  0.393005  0.393004   \n",
       "1  c356067f-132c-4032-a812-4d8a405f6a92   0.289001  0.289001  0.289001   \n",
       "0  877eef99-d724-4928-93ca-8abfb07cf90a   0.292972  0.293027  0.292999   \n",
       "2  de1ad16a-c56d-45ed-ae2a-35da38c4f0aa   0.281538  0.281538  0.281538   \n",
       "\n",
       "   ordinal_fg_bg_acc  planarity_residual_rel  invalid_ratio  overall_score  \n",
       "3                NaN                0.218967       0.010059       0.387085  \n",
       "1                NaN                0.151955       0.019939       0.325722  \n",
       "0                NaN                0.139935       0.025712       0.315776  \n",
       "2                NaN                0.151061       0.025765       0.311085  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, glob, cv2, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "IMG_DIR = \"/content/drive/MyDrive/Depthanythingv2/data/eval\"\n",
    "DEP_DIR = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/depths/raw_da2\"   # relative depth PNG16\n",
    "SEG_INST_DIR = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/seg\"        # optional (uint16 instance ids), can be None\n",
    "REPORT_CSV = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/depth_eval_report.csv\"\n",
    "\n",
    "os.makedirs(os.path.dirname(REPORT_CSV), exist_ok=True)\n",
    "\n",
    "def read_depth_rel(path_png16):\n",
    "    d16 = cv2.imread(path_png16, -1).astype(np.float32)\n",
    "    d = d16 / 65535.0\n",
    "    d[~np.isfinite(d)] = np.nan\n",
    "    return d\n",
    "\n",
    "\n",
    "def sobel_grad(a):\n",
    "    a = np.asarray(a, dtype=np.float32)  # <-- ensure float32\n",
    "    gx = cv2.Sobel(a, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(a, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    return np.sqrt(gx*gx + gy*gy)\n",
    "\n",
    "def edge_alignment_metric(rgb, depth_rel):\n",
    "    # RGB -> edges\n",
    "    g_img = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
    "    g_img = cv2.GaussianBlur(g_img, (3,3), 0.8)\n",
    "    e_img = sobel_grad(g_img)\n",
    "\n",
    "    # Depth -> edges (normalize safely, keep float32)\n",
    "    x = depth_rel.copy()\n",
    "    x = x.astype(np.float32)               # <-- ensure float32\n",
    "    x[np.isnan(x)] = 0\n",
    "    if (x>0).sum() < 10:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    p1, p99 = np.percentile(x[x>0], (1,99))\n",
    "    x = (x - np.float32(p1)) / max(np.float32(1e-6), np.float32(p99 - p1))\n",
    "    x = np.clip(x, 0, 1).astype(np.float32)  # <-- ensure float32\n",
    "    e_depth = sobel_grad(x)\n",
    "\n",
    "    def binarize(e, pct=88):\n",
    "        th = np.percentile(e, pct)\n",
    "        return (e >= th).astype(np.uint8)\n",
    "\n",
    "    Eimg = binarize(e_img, 88)\n",
    "    Edep = binarize(e_depth, 88)\n",
    "\n",
    "    inter = (Eimg & Edep).sum()\n",
    "    dep_sum = Edep.sum() + 1e-6\n",
    "    img_sum = Eimg.sum() + 1e-6\n",
    "    prec = inter / dep_sum\n",
    "    rec  = inter / img_sum\n",
    "    f1   = 2*prec*rec / max(1e-6, (prec+rec))\n",
    "    return float(prec), float(rec), float(f1)\n",
    "\n",
    "\n",
    "def load_inst_mask(path):\n",
    "    if (path is None) or (not os.path.exists(path)):\n",
    "        return None\n",
    "    m = cv2.imread(path, -1)\n",
    "    if m is None:\n",
    "        return None\n",
    "    return m\n",
    "\n",
    "def ordinal_person_vs_bg(depth_rel, inst=None):\n",
    "    # Heuristic: person vs background ordering\n",
    "    if inst is None:\n",
    "        return np.nan\n",
    "    # assume person ids are >0; if you have class ids, you can refine later\n",
    "    # For now: treat the largest non-zero instance as foreground “person”-like\n",
    "    ids, counts = np.unique(inst[inst>0], return_counts=True)\n",
    "    if len(ids)==0:\n",
    "        return np.nan\n",
    "    fg_id = ids[np.argmax(counts)]\n",
    "    fg = (inst==fg_id)\n",
    "    if fg.sum() < 200:  # too tiny, skip\n",
    "        return np.nan\n",
    "    # background approx: farthest 20% depth pixels outside fg\n",
    "    bg = (~fg) & (depth_rel>0)\n",
    "    if bg.sum() < 200:\n",
    "        return np.nan\n",
    "    # sample N pairs\n",
    "    N = 2000\n",
    "    ys, xs = np.where(fg)\n",
    "    idx_fg = np.random.choice(len(xs), size=min(N, len(xs)), replace=False)\n",
    "    ys2, xs2 = np.where(bg)\n",
    "    idx_bg = np.random.choice(len(xs2), size=min(N, len(xs2)), replace=False)\n",
    "    df = depth_rel[ys[idx_fg], xs[idx_fg]]\n",
    "    db = depth_rel[ys2[idx_bg], xs2[idx_bg]]\n",
    "    # Nearer should have larger relative depth if we used inverse earlier,\n",
    "    # but here depth_rel is arbitrary. We'll check ratio ordering: fg should be \"closer\"\n",
    "    # Assume fg is closer => depth_rel[fg] < depth_rel[bg] (or > depending on model)\n",
    "    # Robust trick: compare ranks using median thresholds\n",
    "    med_fg = np.median(df[df>0]) if np.any(df>0) else 0\n",
    "    med_bg = np.median(db[db>0]) if np.any(db>0) else 0\n",
    "    # we don't know orientation; pick the orientation that gives higher accuracy\n",
    "    acc1 = np.mean(df < db)\n",
    "    acc2 = np.mean(df > db)\n",
    "    return float(max(acc1, acc2))\n",
    "\n",
    "def planarity_residual(depth_rel, rgb=None, inst=None):\n",
    "    # Fit a dominant plane on the largest smooth region (no GT needed).\n",
    "    # Normalize by median depth to make it scale-free.\n",
    "    H,W = depth_rel.shape\n",
    "    z = depth_rel.copy()\n",
    "    mask = z>0\n",
    "    if mask.sum() < 1000:\n",
    "        return np.nan\n",
    "    ys, xs = np.where(mask)\n",
    "    # Downsample points for speed\n",
    "    sel = np.random.choice(len(xs), size=min(20000, len(xs)), replace=False)\n",
    "    xs, ys = xs[sel], ys[sel]\n",
    "    zz = z[ys, xs]\n",
    "\n",
    "    # Backproject with a default K (consistent for relative comparisons)\n",
    "    f = 1.2*max(W,H); cx, cy = W/2, H/2\n",
    "    X = (xs - cx) * zz / f\n",
    "    Y = (ys - cy) * zz / f\n",
    "    pts = np.stack([X,Y,zz], axis=1)\n",
    "\n",
    "    # RANSAC plane fit\n",
    "    # Plane ax+by+cz+d=0; solve by SVD on random 3-pt samples; keep best inliers\n",
    "    best_rmse = None\n",
    "    rng = np.random.default_rng(0)\n",
    "    for _ in range(100):\n",
    "        i = rng.choice(len(pts), 3, replace=False)\n",
    "        P = pts[i]\n",
    "        v1 = P[1]-P[0]; v2 = P[2]-P[0]\n",
    "        n = np.cross(v1, v2)\n",
    "        if np.linalg.norm(n) < 1e-8:\n",
    "            continue\n",
    "        n = n / np.linalg.norm(n)\n",
    "        d = -np.dot(n, P[0])\n",
    "        # distance to plane\n",
    "        dist = np.abs(pts @ n + d)\n",
    "        rmse = np.sqrt(np.mean(dist**2))\n",
    "        if (best_rmse is None) or (rmse < best_rmse):\n",
    "            best_rmse = rmse\n",
    "    if best_rmse is None:\n",
    "        return np.nan\n",
    "    med_z = np.median(zz[zz>0]) if np.any(zz>0) else 1.0\n",
    "    return float(best_rmse / max(1e-6, med_z))  # scale-free residual\n",
    "\n",
    "def invalid_ratio(depth_rel):\n",
    "    return float(1.0 - (depth_rel>0).mean())\n",
    "\n",
    "rows=[]\n",
    "paths = sorted([p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
    "                for p in glob.glob(os.path.join(IMG_DIR, ext))])\n",
    "\n",
    "for p_rgb in tqdm(paths):\n",
    "    stem = os.path.splitext(os.path.basename(p_rgb))[0]\n",
    "    p_dep = os.path.join(DEP_DIR, f\"{stem}.png\")\n",
    "    if not os.path.exists(p_dep):\n",
    "        continue\n",
    "    rgb = cv2.imread(p_rgb)\n",
    "    depth = read_depth_rel(p_dep)\n",
    "\n",
    "    prec, rec, f1 = edge_alignment_metric(rgb, depth)\n",
    "\n",
    "    inst = None\n",
    "    p_inst = os.path.join(SEG_INST_DIR, f\"{stem}_inst.png\") if SEG_INST_DIR else None\n",
    "    if p_inst and os.path.exists(p_inst):\n",
    "        inst = load_inst_mask(p_inst)\n",
    "\n",
    "    ord_acc = ordinal_person_vs_bg(depth, inst)  # NaN if no masks\n",
    "\n",
    "    plan_res = planarity_residual(depth, rgb, inst)\n",
    "    inv_pct = invalid_ratio(depth)\n",
    "\n",
    "    rows.append([stem, prec, rec, f1, ord_acc, plan_res, inv_pct])\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"img_id\",\"edge_prec\",\"edge_rec\",\"edge_f1\",\"ordinal_fg_bg_acc\",\"planarity_residual_rel\",\"invalid_ratio\"\n",
    "])\n",
    "\n",
    "# Simple overall score (higher is better):\n",
    "# Edge F1 (40%), Ordinal (30%), Planarity (20%, lower is better), Invalid (10%, lower is better)\n",
    "def score_row(r):\n",
    "    f1 = np.nan_to_num(r.edge_f1, nan=0.0)\n",
    "    ordv = np.nan_to_num(r.ordinal_fg_bg_acc, nan=0.5)  # if unknown, neutral\n",
    "    plan = np.nan_to_num(r.planarity_residual_rel, nan=0.1)\n",
    "    inv  = np.nan_to_num(r.invalid_ratio, nan=0.0)\n",
    "    # Convert “lower is better” to [0,1]\n",
    "    plan_ok = np.clip(1.0 - (plan/0.1), 0, 1)   # 0.1 ~ “ok” residual\n",
    "    inv_ok  = np.clip(1.0 - (inv/0.05), 0, 1)   # 5% invalid budget\n",
    "    return 0.4*f1 + 0.3*ordv + 0.2*plan_ok + 0.1*inv_ok\n",
    "\n",
    "df[\"overall_score\"] = df.apply(score_row, axis=1)\n",
    "df.to_csv(REPORT_CSV, index=False)\n",
    "df.sort_values(\"overall_score\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f1daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average overall_score = 0.331 ± 0.040\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Depthanythingv2/output-vits/depth_eval_report.csv\")\n",
    "mean = df[\"overall_score\"].mean()\n",
    "std = df[\"overall_score\"].std()\n",
    "print(f\"Average overall_score = {mean:.3f} ± {std:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
