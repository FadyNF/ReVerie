{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acf7b46",
   "metadata": {},
   "source": [
    "### Code (vitb model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR  = \"/content/drive/MyDrive/Depthanythingv2/data/eval\"  # your test images\n",
    "REPO     = \"/content/Depth-Anything-V2\"\n",
    "CKPT     = f\"{REPO}/checkpoints/depth_anything_v2_vitb.pth\"     # vitb weights\n",
    "OUT_REL  = \"/content/drive/MyDrive/Depthanythingv2/output-vitb/depths/raw_da2_vitb896_tta\"\n",
    "OUT_VIZ  = \"/content/drive/MyDrive/Depthanythingv2/output-vitb/depths/viz_da2_vitb896_tta\"\n",
    "\n",
    "import os, sys\n",
    "os.makedirs(OUT_REL, exist_ok=True); os.makedirs(OUT_VIZ, exist_ok=True)\n",
    "sys.path.append(REPO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10922ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/Depth-Anything-V2/checkpoints/depth_anything_v2_vitb.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipython-input-1832399625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     10\u001b[0m     \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m192\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m---> 12\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCKPT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n",
      "\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n",
      "\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n",
      "\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Depth-Anything-V2/checkpoints/depth_anything_v2_vitb.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.backends.cudnn.benchmark = True  # speed on fixed input shapes\n",
    "\n",
    "model = DepthAnythingV2(\n",
    "    encoder='vitb',\n",
    "    features=128,\n",
    "    out_channels=[96, 192, 384, 768]\n",
    ")\n",
    "state = torch.load(CKPT, map_location='cpu')\n",
    "model.load_state_dict(state, strict=True)\n",
    "model = model.to(DEVICE).eval()\n",
    "\n",
    "print(\"Loaded vitb on\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, glob, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def letterbox(img, target=896):\n",
    "    h, w = img.shape[:2]\n",
    "    s = target / max(h, w)\n",
    "    nh, nw = int(round(h*s)), int(round(w*s))\n",
    "    img_r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_CUBIC)\n",
    "    top = (target - nh)//2; bottom = target - nh - top\n",
    "    left = (target - nw)//2; right = target - nw - left\n",
    "    img_p = cv2.copyMakeBorder(img_r, top,bottom,left,right, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
    "    return img_p, (top,bottom,left,right), (h,w)\n",
    "\n",
    "def unletterbox(arr, pads, orig_hw):\n",
    "    top,bottom,left,right = pads\n",
    "    arr = arr[top:arr.shape[0]-bottom, left:arr.shape[1]-right]\n",
    "    return cv2.resize(arr, (orig_hw[1], orig_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_tta_rel(img_bgr):\n",
    "    \"\"\"Return relative depth (float32) using flip-TTA + AMP when on CUDA.\"\"\"\n",
    "    if DEVICE == 'cuda':\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            d0 = model.infer_image(img_bgr).astype(np.float32)\n",
    "            d1 = model.infer_image(cv2.flip(img_bgr, 1)).astype(np.float32)\n",
    "    else:\n",
    "        d0 = model.infer_image(img_bgr).astype(np.float32)\n",
    "        d1 = model.infer_image(cv2.flip(img_bgr, 1)).astype(np.float32)\n",
    "    d1 = np.flip(d1, axis=1)\n",
    "    return (0.5*(d0 + d1)).astype(np.float32)\n",
    "\n",
    "def to_u16_rel(depth):\n",
    "    \"\"\"Robust 16-bit scaling for relative depth; avoids exact zeros.\"\"\"\n",
    "    d = depth.astype(np.float32)\n",
    "    d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if (d>0).sum() < 10:\n",
    "        return np.zeros_like(d, np.uint16)\n",
    "    p1, p99 = np.percentile(d[d>0], (1,99))\n",
    "    p1 = np.float32(p1); p99 = np.float32(p99)\n",
    "    d = np.clip(d, p1, p99)\n",
    "    d = (d - p1) / max(np.float32(1e-6), (p99 - p1))\n",
    "    d = np.clip(d, np.float32(1e-6), np.float32(1.0))\n",
    "    return (d * np.float32(65535.0)).astype(np.uint16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e988f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
    "               for p in glob.glob(os.path.join(IMG_DIR, ext)))\n",
    "\n",
    "for p in tqdm(paths):\n",
    "    img = cv2.imread(p)\n",
    "    img_p, pads, orig_hw = letterbox(img, 896)\n",
    "    d_rel_p = infer_tta_rel(img_p)                 # float32\n",
    "    d_rel   = unletterbox(d_rel_p, pads, orig_hw)  # back to original size\n",
    "\n",
    "    rel16 = to_u16_rel(d_rel)\n",
    "    stem = os.path.splitext(os.path.basename(p))[0]\n",
    "    cv2.imwrite(f\"{OUT_REL}/{stem}.png\", rel16)\n",
    "\n",
    "    viz = cv2.applyColorMap(255 - (rel16//256).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
    "    cv2.imwrite(f\"{OUT_VIZ}/{stem}.jpg\", viz)\n",
    "\n",
    "print(\"Saved:\", len(paths), \"depth maps to\", OUT_REL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f929cb",
   "metadata": {},
   "source": [
    "### Evaluation (vitb model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b50796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, cv2, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "IMG_DIR = IMG_DIR\n",
    "DEP_DIR = OUT_REL\n",
    "REPORT  = \"/content/drive/MyDrive/Depthanythingv2/output-vitb/depth_eval_report_vitb896_tta.csv\"\n",
    "os.makedirs(os.path.dirname(REPORT), exist_ok=True)\n",
    "\n",
    "def read_depth_rel(png16):\n",
    "    d16 = cv2.imread(png16, -1).astype(np.float32)\n",
    "    d = d16 / np.float32(65535.0)\n",
    "    d[~np.isfinite(d)] = np.nan\n",
    "    return d.astype(np.float32)\n",
    "\n",
    "def sobel_grad(a):\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    gx = cv2.Sobel(a, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(a, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    return np.sqrt(gx*gx + gy*gy).astype(np.float32)\n",
    "\n",
    "def edge_metrics(rgb, depth_rel):\n",
    "    g = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY).astype(np.float32)/255.0\n",
    "    g = cv2.GaussianBlur(g,(3,3),0.8).astype(np.float32)\n",
    "    e_img = sobel_grad(g)\n",
    "\n",
    "    x = depth_rel.astype(np.float32); x[np.isnan(x)] = 0.0\n",
    "    if (x>0).sum()<10: return 0.0,0.0,0.0\n",
    "    p1,p99 = np.percentile(x[x>0],(1,99)); p1=np.float32(p1); p99=np.float32(p99)\n",
    "    x = np.clip((x-p1)/max(np.float32(1e-6), (p99-p1)), 0, 1).astype(np.float32)\n",
    "    e_dep = sobel_grad(x)\n",
    "\n",
    "    t1 = np.float32(np.percentile(e_img,88))\n",
    "    t2 = np.float32(np.percentile(e_dep,88))\n",
    "    E1 = (e_img>=t1).astype(np.uint8); E2 = (e_dep>=t2).astype(np.uint8)\n",
    "\n",
    "    inter = (E1 & E2).sum(); dep_sum = E2.sum()+1e-6; img_sum = E1.sum()+1e-6\n",
    "    prec = inter/dep_sum; rec = inter/img_sum\n",
    "    f1 = 2*prec*rec/max(1e-6,(prec+rec))\n",
    "    return float(prec), float(rec), float(f1)\n",
    "\n",
    "def planarity(depth_rel):\n",
    "    z = depth_rel.astype(np.float32)\n",
    "    mask = np.isfinite(z) & (z>0)\n",
    "    if mask.sum() < 1000: return np.nan\n",
    "    ys,xs = np.where(mask)\n",
    "    sel = np.random.choice(len(xs), size=min(20000,len(xs)), replace=False)\n",
    "    xs,ys = xs[sel],ys[sel]; zz = z[ys,xs].astype(np.float32)\n",
    "\n",
    "    H,W = z.shape; f=np.float32(1.2*max(W,H)); cx=np.float32(W/2); cy=np.float32(H/2)\n",
    "    X=(xs.astype(np.float32)-cx)*zz/f; Y=(ys.astype(np.float32)-cy)*zz/f\n",
    "    P=np.stack([X,Y,zz],1).astype(np.float32)\n",
    "\n",
    "    best=None; rng=np.random.default_rng(0)\n",
    "    for _ in range(100):\n",
    "        i=rng.choice(len(P),3,replace=False)\n",
    "        v1=P[i[1]]-P[i[0]]; v2=P[i[2]]-P[i[0]]\n",
    "        n=np.cross(v1,v2).astype(np.float32); n_norm=np.linalg.norm(n)\n",
    "        if n_norm<1e-8: continue\n",
    "        n/=n_norm; d0=-np.dot(n,P[i[0]]).astype(np.float32)\n",
    "        dist=np.abs(P@n + d0).astype(np.float32)\n",
    "        rmse=float(np.sqrt(np.mean(dist**2)))\n",
    "        if (best is None) or (rmse<best): best=rmse\n",
    "    if best is None: return np.nan\n",
    "    med=float(np.median(zz[zz>0])) if np.any(zz>0) else 1.0\n",
    "    return float(best/max(1e-6, med))\n",
    "\n",
    "def invalid_ratio(depth_rel):\n",
    "    return float(np.isnan(depth_rel).mean())\n",
    "\n",
    "rows=[]\n",
    "img_paths = sorted(p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
    "                   for p in glob.glob(os.path.join(IMG_DIR, ext)))\n",
    "for p_rgb in tqdm(img_paths):\n",
    "    stem = os.path.splitext(os.path.basename(p_rgb))[0]\n",
    "    p_dep = os.path.join(DEP_DIR, f\"{stem}.png\")\n",
    "    if not os.path.exists(p_dep): continue\n",
    "    rgb = cv2.imread(p_rgb); depth = read_depth_rel(p_dep)\n",
    "    prec,rec,f1 = edge_metrics(rgb, depth)\n",
    "    plan = planarity(depth); inv = invalid_ratio(depth)\n",
    "    rows.append([stem,prec,rec,f1,plan,inv])\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "df = pd.DataFrame(rows, columns=[\"img_id\",\"edge_prec\",\"edge_rec\",\"edge_f1\",\"planarity_residual_rel\",\"invalid_ratio\"])\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"No matches: check that PNGs in OUT_REL share basenames with images.\")\n",
    "\n",
    "f1   = df[\"edge_f1\"].astype(float).fillna(0.0)\n",
    "plan = df[\"planarity_residual_rel\"].astype(float).fillna(0.1)\n",
    "inv  = df[\"invalid_ratio\"].astype(float).fillna(0.0)\n",
    "plan_ok = np.clip(1.0 - (plan/0.1), 0.0, 1.0)\n",
    "inv_ok  = np.clip(1.0 - (inv /0.05), 0.0, 1.0)\n",
    "df[\"overall_score\"] = 0.6*f1 + 0.25*plan_ok + 0.15*inv_ok\n",
    "\n",
    "df.to_csv(REPORT, index=False)\n",
    "print(f\"Average overall_score = {df['overall_score'].mean():.3f} ± {df['overall_score'].std():.3f}\")\n",
    "df.sort_values(\"overall_score\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Depthanythingv2/output-vitb/depth_eval_report.csv\")\n",
    "mean = df[\"overall_score\"].mean()\n",
    "std = df[\"overall_score\"].std()\n",
    "print(f\"Average overall_score = {mean:.3f} ± {std:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
