{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cf39c9",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SUPABASE_URL\"] = \"https://viqawhbbhlkbjxntmwpn.supabase.co\"\n",
    "os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InZpcWF3aGJiaGxrYmp4bnRtd3BuIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTYzNjEwMywiZXhwIjoyMDg1MjEyMTAzfQ.DvUNXsw8gwdxhiG71QPYNVFhQIYRcDQuP9QC2VOiCu0\"  # backend-only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd05b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK URL: https://viqawhbbhlkbjxntmwpn.supabase.co/\n",
      "OK client: https://viqawhbbhlkbjxntmwpn.supabase.co/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from supabase import create_client\n",
    "\n",
    "# 1) Force trailing slash\n",
    "os.environ[\"SUPABASE_URL\"] = os.environ[\"SUPABASE_URL\"].strip()\n",
    "if not os.environ[\"SUPABASE_URL\"].endswith(\"/\"):\n",
    "    os.environ[\"SUPABASE_URL\"] += \"/\"\n",
    "\n",
    "# 2) Recreate client AFTER fixing\n",
    "supabase = create_client(os.environ[\"SUPABASE_URL\"], os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"])\n",
    "\n",
    "print(\"OK URL:\", os.environ[\"SUPABASE_URL\"])\n",
    "print(\"OK client:\", supabase.supabase_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1f065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK DB: scene_runs + pipeline_outputs exist\n"
     ]
    }
   ],
   "source": [
    "# Check DB tables exist\n",
    "try:\n",
    "    r = supabase.table(\"scene_runs\").select(\"id\").limit(1).execute()\n",
    "    r2 = supabase.table(\"pipeline_outputs\").select(\"id\").limit(1).execute()\n",
    "    print(\"OK DB: scene_runs + pipeline_outputs exist\")\n",
    "except Exception as e:\n",
    "    print(\"DB FAIL:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e51107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK Storage upload: runs/_debug/test.txt\n"
     ]
    }
   ],
   "source": [
    "BUCKET = \"Pipeline\"\n",
    "\n",
    "test_path = \"runs/_debug/test.txt\"\n",
    "content = b\"hello supabase\"\n",
    "\n",
    "supabase.storage.from_(BUCKET).upload(\n",
    "    path=test_path,\n",
    "    file=content,\n",
    "    file_options={\"content-type\":\"text/plain\",\"upsert\":\"true\"}\n",
    ")\n",
    "\n",
    "print(\"OK Storage upload:\", test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf4090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK run_id: aea38f28-476b-4d89-8cc7-707b83611ef3\n",
      "OK storage input upload: runs/aea38f28-476b-4d89-8cc7-707b83611ef3/input/original.jpg\n",
      "OK DB row upserted for input\n"
     ]
    }
   ],
   "source": [
    "import cv2, os\n",
    "\n",
    "BUCKET = \"Pipeline\"\n",
    "\n",
    "IMG_PATH = \"/content/drive/MyDrive/Depthanythingv2/data/eval/402db54d-53c8-4392-a5dc-b2e5fb6c337a.jpg\"\n",
    "\n",
    "# 1) Create run\n",
    "run = supabase.table(\"scene_runs\").insert({\n",
    "    \"scene_name\": \"debug_depth_run\",\n",
    "    \"notes\": \"step4: input upload only\"\n",
    "}).execute()\n",
    "\n",
    "run_id = run.data[0][\"id\"]\n",
    "print(\"OK run_id:\", run_id)\n",
    "\n",
    "# 2) Read image\n",
    "img = cv2.imread(IMG_PATH)\n",
    "if img is None:\n",
    "    raise ValueError(\"Image not found/readable: \" + IMG_PATH)\n",
    "\n",
    "# 3) Upload image to storage under run folder\n",
    "input_path = f\"runs/{run_id}/input/original.jpg\"\n",
    "\n",
    "ok, buf = cv2.imencode(\".jpg\", img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "if not ok:\n",
    "    raise RuntimeError(\"JPEG encode failed\")\n",
    "\n",
    "supabase.storage.from_(BUCKET).upload(\n",
    "    path=input_path,\n",
    "    file=buf.tobytes(),\n",
    "    file_options={\"content-type\":\"image/jpeg\",\"upsert\":\"true\"}\n",
    ")\n",
    "print(\"OK storage input upload:\", input_path)\n",
    "\n",
    "# 4) Upsert DB row in pipeline_outputs\n",
    "h, w = img.shape[:2]\n",
    "row = {\n",
    "    \"run_id\": run_id,\n",
    "    \"stage\": \"input\",\n",
    "    \"file_role\": \"original_image\",\n",
    "    \"bucket\": BUCKET,\n",
    "    \"path\": input_path,\n",
    "    \"mime_type\": \"image/jpeg\",\n",
    "    \"meta\": {\"width\": w, \"height\": h, \"source\": \"colab_step4\"}\n",
    "}\n",
    "supabase.table(\"pipeline_outputs\").upsert(row).execute()\n",
    "\n",
    "print(\"OK DB row upserted for input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d87160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letterbox: False\n",
      "unletterbox: False\n",
      "infer_tta_rel: False\n",
      "to_u16_rel: False\n"
     ]
    }
   ],
   "source": [
    "print(\"letterbox:\", \"letterbox\" in globals())\n",
    "print(\"unletterbox:\", \"unletterbox\" in globals())\n",
    "print(\"infer_tta_rel:\", \"infer_tta_rel\" in globals())\n",
    "print(\"to_u16_rel:\", \"to_u16_rel\" in globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa29143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158a137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK model loaded on cuda\n",
      "OK helpers defined: letterbox/unletterbox/infer_tta_rel/to_u16_rel\n"
     ]
    }
   ],
   "source": [
    "import os, sys, cv2, numpy as np, torch\n",
    "\n",
    "# --- paths: update if yours differ ---\n",
    "REPO = \"/content/drive/MyDrive/Depthanythingv2\"\n",
    "CKPT = f\"{REPO}/checkpoints/depth_anything_v2_vitl.pth\"\n",
    "\n",
    "# Make sure we can import the library\n",
    "if REPO not in sys.path:\n",
    "    sys.path.append(REPO)\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# vitl config\n",
    "model = DepthAnythingV2(encoder=\"vitl\", features=256, out_channels=[256, 512, 1024, 1024])\n",
    "state = torch.load(CKPT, map_location=\"cpu\")\n",
    "model.load_state_dict(state, strict=True)\n",
    "model = model.to(DEVICE).eval()\n",
    "print(\"OK model loaded on\", DEVICE)\n",
    "\n",
    "# ---- required helpers ----\n",
    "def letterbox(img, target=896):\n",
    "    h, w = img.shape[:2]\n",
    "    s = target / max(h, w)\n",
    "    nh, nw = int(round(h * s)), int(round(w * s))\n",
    "    img_r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_CUBIC)\n",
    "    top = (target - nh) // 2; bottom = target - nh - top\n",
    "    left = (target - nw) // 2; right = target - nw - left\n",
    "    img_p = cv2.copyMakeBorder(img_r, top, bottom, left, right,\n",
    "                               cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    return img_p, (top, bottom, left, right), (h, w)\n",
    "\n",
    "def unletterbox(arr, pads, orig_hw):\n",
    "    top, bottom, left, right = pads\n",
    "    arr = arr[top:arr.shape[0]-bottom, left:arr.shape[1]-right]\n",
    "    return cv2.resize(arr, (orig_hw[1], orig_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_tta_rel(img_bgr):\n",
    "    # Predict and average normal + flipped; keep float32\n",
    "    d0 = model.infer_image(img_bgr).astype(np.float32)\n",
    "    d1 = model.infer_image(cv2.flip(img_bgr, 1)).astype(np.float32)\n",
    "    d1 = np.flip(d1, axis=1)\n",
    "    return (0.5 * (d0 + d1)).astype(np.float32)\n",
    "\n",
    "def to_u16_rel(depth):\n",
    "    d = depth.astype(np.float32)\n",
    "    d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if (d > 0).sum() < 10:\n",
    "        return np.zeros_like(d, dtype=np.uint16)\n",
    "    p1, p99 = np.percentile(d[d > 0], (1, 99))\n",
    "    p1 = np.float32(p1); p99 = np.float32(p99)\n",
    "    d = np.clip(d, p1, p99)\n",
    "    d = (d - p1) / max(np.float32(1e-6), (p99 - p1))\n",
    "    d = np.clip(d, np.float32(1e-6), np.float32(1.0))\n",
    "    return (d * np.float32(65535.0)).astype(np.uint16)\n",
    "\n",
    "print(\"OK helpers defined: letterbox/unletterbox/infer_tta_rel/to_u16_rel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19c441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK run_id: 13a3f2a8-a2f7-4b2f-95ff-f08d662f357e\n",
      "OK input stored + DB row\n",
      "OK depth inferred\n",
      "OK depth outputs uploaded + DB rows\n",
      "RUN DONE: 13a3f2a8-a2f7-4b2f-95ff-f08d662f357e\n",
      "Input: runs/13a3f2a8-a2f7-4b2f-95ff-f08d662f357e/input/original.jpg\n",
      "Depth PNG: runs/13a3f2a8-a2f7-4b2f-95ff-f08d662f357e/depth/depth_rel_u16.png\n",
      "Depth Viz: runs/13a3f2a8-a2f7-4b2f-95ff-f08d662f357e/depth/depth_viz.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "BUCKET = \"Pipeline\"\n",
    "IMG_PATH = \"/content/drive/MyDrive/Depthanythingv2/data/eval/c2dfaff7-cab9-4f3c-9a83-71d88d36526c.JPG\"\n",
    "\n",
    "# 1) Create run\n",
    "run = supabase.table(\"scene_runs\").insert({\n",
    "    \"scene_name\": \"depth_trial\",\n",
    "    \"notes\": \"step5: depth infer + upload\"\n",
    "}).execute()\n",
    "run_id = run.data[0][\"id\"]\n",
    "print(\"OK run_id:\", run_id)\n",
    "\n",
    "# 2) Read image\n",
    "img = cv2.imread(IMG_PATH)\n",
    "if img is None:\n",
    "    raise ValueError(\"Image not found/readable: \" + IMG_PATH)\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "# 3) Upload input\n",
    "input_path = f\"runs/{run_id}/input/original.jpg\"\n",
    "ok, buf = cv2.imencode(\".jpg\", img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "if not ok:\n",
    "    raise RuntimeError(\"JPEG encode failed\")\n",
    "\n",
    "supabase.storage.from_(BUCKET).upload(\n",
    "    path=input_path,\n",
    "    file=buf.tobytes(),\n",
    "    file_options={\"content-type\":\"image/jpeg\",\"upsert\":\"true\"}\n",
    ")\n",
    "\n",
    "supabase.table(\"pipeline_outputs\").upsert({\n",
    "    \"run_id\": run_id,\n",
    "    \"stage\": \"input\",\n",
    "    \"file_role\": \"original_image\",\n",
    "    \"bucket\": BUCKET,\n",
    "    \"path\": input_path,\n",
    "    \"mime_type\": \"image/jpeg\",\n",
    "    \"meta\": {\"width\": w, \"height\": h}\n",
    "}).execute()\n",
    "print(\"OK input stored + DB row\")\n",
    "\n",
    "# 4) Depth inference\n",
    "img_p, pads, orig_hw = letterbox(img, 896)\n",
    "depth_rel_p = infer_tta_rel(img_p)\n",
    "depth_rel = unletterbox(depth_rel_p, pads, orig_hw).astype(np.float32)\n",
    "\n",
    "rel16 = to_u16_rel(depth_rel)\n",
    "viz = cv2.applyColorMap(255 - (rel16 // 256).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
    "print(\"OK depth inferred\")\n",
    "\n",
    "# 5) Upload depth outputs\n",
    "depth_png_path = f\"runs/{run_id}/depth/depth_rel_u16.png\"\n",
    "depth_viz_path = f\"runs/{run_id}/depth/depth_viz.jpg\"\n",
    "\n",
    "ok, buf_png = cv2.imencode(\".png\", rel16)\n",
    "if not ok:\n",
    "    raise RuntimeError(\"Depth PNG encode failed\")\n",
    "ok, buf_viz = cv2.imencode(\".jpg\", viz, [int(cv2.IMWRITE_JPEG_QUALITY), 92])\n",
    "if not ok:\n",
    "    raise RuntimeError(\"Viz JPG encode failed\")\n",
    "\n",
    "supabase.storage.from_(BUCKET).upload(\n",
    "    path=depth_png_path,\n",
    "    file=buf_png.tobytes(),\n",
    "    file_options={\"content-type\":\"image/png\",\"upsert\":\"true\"}\n",
    ")\n",
    "supabase.storage.from_(BUCKET).upload(\n",
    "    path=depth_viz_path,\n",
    "    file=buf_viz.tobytes(),\n",
    "    file_options={\"content-type\":\"image/jpeg\",\"upsert\":\"true\"}\n",
    ")\n",
    "\n",
    "# 6) DB upserts for outputs\n",
    "supabase.table(\"pipeline_outputs\").upsert({\n",
    "    \"run_id\": run_id,\n",
    "    \"stage\": \"depth\",\n",
    "    \"file_role\": \"depth_map_png16\",\n",
    "    \"bucket\": BUCKET,\n",
    "    \"path\": depth_png_path,\n",
    "    \"mime_type\": \"image/png\",\n",
    "    \"meta\": {\"encoding\":\"u16_rel\", \"model\":\"DA2_vitl\", \"tta\": True, \"input_size\": 896}\n",
    "}).execute()\n",
    "\n",
    "supabase.table(\"pipeline_outputs\").upsert({\n",
    "    \"run_id\": run_id,\n",
    "    \"stage\": \"depth\",\n",
    "    \"file_role\": \"depth_viz\",\n",
    "    \"bucket\": BUCKET,\n",
    "    \"path\": depth_viz_path,\n",
    "    \"mime_type\": \"image/jpeg\",\n",
    "    \"meta\": {\"colormap\":\"inferno\"}\n",
    "}).execute()\n",
    "\n",
    "print(\"OK depth outputs uploaded + DB rows\")\n",
    "print(\"RUN DONE:\", run_id)\n",
    "print(\"Input:\", input_path)\n",
    "print(\"Depth PNG:\", depth_png_path)\n",
    "print(\"Depth Viz:\", depth_viz_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ddbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e73eaf73",
   "metadata": {},
   "source": [
    "# Integrating with our actual erd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce66693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SUPABASE_URL\"] = \"https://gkfrvfepccknnoigicxz.supabase.co\"\n",
    "os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdrZnJ2ZmVwY2Nrbm5vaWdpY3h6Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2OTY4MzkxNCwiZXhwIjoyMDg1MjU5OTE0fQ.T9qJ1h4vQinNy6A9obcNW35Np945tTa3t-u_ZmMtWWo\"  # backend-only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cv2, numpy as np, torch\n",
    "from supabase import create_client\n",
    "\n",
    "# ---------- Supabase init ----------\n",
    "os.environ[\"SUPABASE_URL\"] = os.environ[\"SUPABASE_URL\"].strip()\n",
    "if not os.environ[\"SUPABASE_URL\"].endswith(\"/\"):\n",
    "    os.environ[\"SUPABASE_URL\"] += \"/\"\n",
    "\n",
    "supabase = create_client(os.environ[\"SUPABASE_URL\"], os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"pipeline\"  # <- pick ONE, lowercase recommended\n",
    "\n",
    "def upload_bytes(path, content_bytes, content_type):\n",
    "    supabase.storage.from_(BUCKET).upload(\n",
    "        path=path,\n",
    "        file=content_bytes,\n",
    "        file_options={\"content-type\": content_type, \"upsert\": \"true\"}\n",
    "    )\n",
    "\n",
    "def encode_jpg(bgr, quality=95):\n",
    "    ok, buf = cv2.imencode(\".jpg\", bgr, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
    "    if not ok: raise RuntimeError(\"JPEG encode failed\")\n",
    "    return buf.tobytes()\n",
    "\n",
    "def encode_png_u16(u16):\n",
    "    ok, buf = cv2.imencode(\".png\", u16)\n",
    "    if not ok: raise RuntimeError(\"PNG encode failed\")\n",
    "    return buf.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa178dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using repo_root: /content/drive/MyDrive/Depthanythingv2/Depth-Anything-V2\n",
      "sys.path[0]: /content/drive/MyDrive/Depthanythingv2/Depth-Anything-V2\n",
      "OK import DepthAnythingV2\n"
     ]
    }
   ],
   "source": [
    "import sys, os, glob\n",
    "\n",
    "REPO = \"/content/drive/MyDrive/Depthanythingv2\"\n",
    "dirs = glob.glob(REPO + \"/**/depth_anything_v2\", recursive=True)\n",
    "\n",
    "if not dirs:\n",
    "    raise RuntimeError(\"Can't find depth_anything_v2 folder anywhere under REPO.\")\n",
    "\n",
    "module_dir = dirs[0]                      # .../depth_anything_v2\n",
    "repo_root = os.path.dirname(module_dir)   # the folder that CONTAINS depth_anything_v2\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "\n",
    "print(\"Using repo_root:\", repo_root)\n",
    "print(\"sys.path[0]:\", sys.path[0])\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "print(\"OK import DepthAnythingV2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "# ---------- DepthAnythingV2 load ----------\n",
    "REPO = \"/content/drive/MyDrive/Depthanythingv2\"\n",
    "CKPT = f\"{REPO}/checkpoints/depth_anything_v2_vitl.pth\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = DepthAnythingV2(encoder=\"vitl\", features=256, out_channels=[256, 512, 1024, 1024])\n",
    "state = torch.load(CKPT, map_location=\"cpu\")\n",
    "model.load_state_dict(state, strict=True)\n",
    "model = model.to(DEVICE).eval()\n",
    "print(\"OK model loaded on\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e1cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers ----------\n",
    "def letterbox(img, target=896):\n",
    "    h, w = img.shape[:2]\n",
    "    s = target / max(h, w)\n",
    "    nh, nw = int(round(h * s)), int(round(w * s))\n",
    "    img_r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_CUBIC)\n",
    "    top = (target - nh) // 2; bottom = target - nh - top\n",
    "    left = (target - nw) // 2; right = target - nw - left\n",
    "    img_p = cv2.copyMakeBorder(img_r, top, bottom, left, right,\n",
    "                               cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    return img_p, (top, bottom, left, right), (h, w)\n",
    "\n",
    "def unletterbox(arr, pads, orig_hw):\n",
    "    top, bottom, left, right = pads\n",
    "    arr = arr[top:arr.shape[0]-bottom, left:arr.shape[1]-right]\n",
    "    return cv2.resize(arr, (orig_hw[1], orig_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_tta_rel(img_bgr):\n",
    "    d0 = model.infer_image(img_bgr).astype(np.float32)\n",
    "    d1 = model.infer_image(cv2.flip(img_bgr, 1)).astype(np.float32)\n",
    "    d1 = np.flip(d1, axis=1)\n",
    "    return (0.5 * (d0 + d1)).astype(np.float32)\n",
    "\n",
    "def to_u16_rel(depth):\n",
    "    d = depth.astype(np.float32)\n",
    "    d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if (d > 0).sum() < 10:\n",
    "        return np.zeros_like(d, dtype=np.uint16)\n",
    "    p1, p99 = np.percentile(d[d > 0], (1, 99))\n",
    "    d = np.clip(d, np.float32(p1), np.float32(p99))\n",
    "    d = (d - np.float32(p1)) / max(np.float32(1e-6), (np.float32(p99) - np.float32(p1)))\n",
    "    d = np.clip(d, np.float32(1e-6), np.float32(1.0))\n",
    "    return (d * np.float32(65535.0)).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80108bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- ERD-aligned runner ----------\n",
    "def run_depth_job(image_path, user_id=None, scene_name=\"depth_scene\"):\n",
    "    \"\"\"\n",
    "    ERD-aligned:\n",
    "    - creates vr_scenes\n",
    "    - creates ai_jobs(stage='depth_estimation')\n",
    "    - uploads input + depth outputs to storage\n",
    "    - inserts pipeline_outputs tied to (job_id, vr_scene_id)\n",
    "    \"\"\"\n",
    "\n",
    "    # read image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Failed to read image: \" + image_path)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # 1) create vr_scene (attach user_id if you have it, else leave null by not passing)\n",
    "    scene_payload = {\"scene_url\": None, \"processing_status\": \"processing\"}\n",
    "    if user_id is not None:\n",
    "        scene_payload[\"user_id\"] = user_id\n",
    "\n",
    "    scene = supabase.table(\"vr_scenes\").insert(scene_payload).execute()\n",
    "    vr_scene_id = scene.data[0][\"id\"]\n",
    "\n",
    "    # 2) create ai_job for depth\n",
    "    job = supabase.table(\"ai_jobs\").insert({\n",
    "        \"vr_scene_id\": vr_scene_id,\n",
    "        \"stage\": \"depth_estimation\",\n",
    "        \"status\": \"running\",\n",
    "        \"model_name\": \"DepthAnythingV2\",\n",
    "        \"model_version\": \"vitl\",\n",
    "        \"params\": {\"tta\": True, \"input_size\": 896},\n",
    "        \"attempt\": 1\n",
    "    }).execute()\n",
    "    job_id = job.data[0][\"id\"]\n",
    "\n",
    "    base = f\"scenes/{vr_scene_id}/jobs/{job_id}\"\n",
    "\n",
    "    try:\n",
    "        # 3) upload input\n",
    "        input_uri = f\"{base}/input/original.jpg\"\n",
    "        upload_bytes(input_uri, encode_jpg(img, 95), \"image/jpeg\")\n",
    "\n",
    "        supabase.table(\"pipeline_outputs\").insert({\n",
    "            \"job_id\": job_id,\n",
    "            \"vr_scene_id\": vr_scene_id,\n",
    "            \"output_type\": \"input_image\",\n",
    "            \"uri\": input_uri,\n",
    "            \"mime_type\": \"image/jpeg\",\n",
    "            \"size_bytes\": None,\n",
    "            \"meta\": {\"width\": w, \"height\": h}\n",
    "        }).execute()\n",
    "\n",
    "        # 4) depth inference\n",
    "        img_p, pads, orig_hw = letterbox(img, 896)\n",
    "        depth_rel_p = infer_tta_rel(img_p)\n",
    "        depth_rel = unletterbox(depth_rel_p, pads, orig_hw).astype(np.float32)\n",
    "\n",
    "        rel16 = to_u16_rel(depth_rel)\n",
    "        viz = cv2.applyColorMap(255 - (rel16 // 256).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
    "\n",
    "        # 5) upload outputs\n",
    "        depth_uri = f\"{base}/depth/depth_rel_u16.png\"\n",
    "        viz_uri   = f\"{base}/depth/depth_viz.jpg\"\n",
    "        upload_bytes(depth_uri, encode_png_u16(rel16), \"image/png\")\n",
    "        upload_bytes(viz_uri, encode_jpg(viz, 92), \"image/jpeg\")\n",
    "\n",
    "        supabase.table(\"pipeline_outputs\").insert({\n",
    "            \"job_id\": job_id,\n",
    "            \"vr_scene_id\": vr_scene_id,\n",
    "            \"output_type\": \"depth\",\n",
    "            \"uri\": depth_uri,\n",
    "            \"mime_type\": \"image/png\",\n",
    "            \"size_bytes\": None,\n",
    "            \"meta\": {\"encoding\": \"u16_rel\", \"tta\": True, \"input_size\": 896}\n",
    "        }).execute()\n",
    "\n",
    "        supabase.table(\"pipeline_outputs\").insert({\n",
    "            \"job_id\": job_id,\n",
    "            \"vr_scene_id\": vr_scene_id,\n",
    "            \"output_type\": \"preview\",\n",
    "            \"uri\": viz_uri,\n",
    "            \"mime_type\": \"image/jpeg\",\n",
    "            \"size_bytes\": None,\n",
    "            \"meta\": {\"kind\": \"depth_viz\", \"colormap\": \"inferno\"}\n",
    "        }).execute()\n",
    "\n",
    "        # 6) mark job + scene success\n",
    "        supabase.table(\"ai_jobs\").update({\n",
    "            \"status\": \"succeeded\",\n",
    "            \"finished_at\": \"now()\"\n",
    "        }).eq(\"id\", job_id).execute()\n",
    "\n",
    "        supabase.table(\"vr_scenes\").update({\n",
    "            \"processing_status\": \"succeeded\"\n",
    "        }).eq(\"id\", vr_scene_id).execute()\n",
    "\n",
    "        return {\"vr_scene_id\": vr_scene_id, \"job_id\": job_id, \"input\": input_uri, \"depth\": depth_uri, \"viz\": viz_uri}\n",
    "\n",
    "    except Exception as e:\n",
    "        supabase.table(\"ai_jobs\").update({\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e)\n",
    "        }).eq(\"id\", job_id).execute()\n",
    "\n",
    "        supabase.table(\"vr_scenes\").update({\n",
    "            \"processing_status\": \"failed\"\n",
    "        }).eq(\"id\", vr_scene_id).execute()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57697dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vr_scene_id': '1b7d7887-85da-49f9-bb73-a435d1cc1620',\n",
       " 'job_id': '80d36f06-2834-40ea-9d84-4ccc2fa5c2ea',\n",
       " 'input': 'scenes/1b7d7887-85da-49f9-bb73-a435d1cc1620/jobs/80d36f06-2834-40ea-9d84-4ccc2fa5c2ea/input/original.jpg',\n",
       " 'depth': 'scenes/1b7d7887-85da-49f9-bb73-a435d1cc1620/jobs/80d36f06-2834-40ea-9d84-4ccc2fa5c2ea/depth/depth_rel_u16.png',\n",
       " 'viz': 'scenes/1b7d7887-85da-49f9-bb73-a435d1cc1620/jobs/80d36f06-2834-40ea-9d84-4ccc2fa5c2ea/depth/depth_viz.jpg'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = run_depth_job(\"/content/drive/MyDrive/Segmentation/dataset/157_00150.png\")\n",
    "out\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
