{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d20bb52"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ace50599",
        "outputId": "8eec7280-7c7f-4d9b-d5aa-f87282470795"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DepthAnything/Depth-Anything-V2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmQhGZdv4RJU",
        "outputId": "a1b6bf62-3221-4dd4-a3f3-5b6ee083aed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Depth-Anything-V2' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/Depth-Anything-V2')"
      ],
      "metadata": {
        "id": "OJfIJqQF4XLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/Depth-Anything-V2"
      ],
      "metadata": {
        "id": "dLOicPVk4TM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e28602-de4c-48b0-8223-64f04c8993a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: /content/Depth-Anything-V2: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKVM_YAp4iBc",
        "outputId": "b7f768dd-ccfe-480d-87e3-937edc87daef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, torch\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "print(\"DepthAnythingV2 import OK\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "_0D-uEDA47sX",
        "outputId": "32d4962c-01ab-485e-be7f-3838360578ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy: 2.0.2\n",
            "Torch: 2.9.0+cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'depth_anything_v2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4237806513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NumPy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdepth_anything_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDepthAnythingV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DepthAnythingV2 import OK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'depth_anything_v2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe233a79"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Code (vits model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, sys, cv2, numpy as np, torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "IMG_DIR  = \"/content/drive/MyDrive/Depthanythingv2/data/eval\"\n",
        "REPO     = \"/content/Depth-Anything-V2\"\n",
        "CKPT     = f\"{REPO}/checkpoints/depth_anything_v2_vits.pth\"  # vits checkpoint\n",
        "OUT_REL  = \"/content/drive/MyDrive/Depthanythingv2/output-vits/depths/raw_da2_vits\"\n",
        "OUT_VIZ  = \"/content/drive/MyDrive/Depthanythingv2/output-vits/depths/viz_da2_vits\""
      ],
      "metadata": {
        "id": "L-2LNdAA_44v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85ae9a18"
      },
      "source": [
        "import cv2\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "\n",
        "model_configs = {\n",
        "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
        "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
        "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
        "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
        "}\n",
        "\n",
        "encoder = 'vits' # or 'vits', 'vitb', 'vitg'\n",
        "\n",
        "model = DepthAnythingV2(**model_configs[encoder])\n",
        "model.load_state_dict(torch.load(f'/content/drive/MyDrive/Depthanythingv2/checkpoints/depth_anything_v2_{encoder}.pth', map_location='cpu'))\n",
        "model = model.to(DEVICE).eval()\n",
        "\n",
        " # HxW raw depth map in numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_u16_rel(depth):\n",
        "    \"\"\"Normalize relative depth to 16-bit safely (no exact zeros).\"\"\"\n",
        "    d = depth.astype(np.float32)\n",
        "    d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    if (d > 0).sum() < 10:\n",
        "        return np.zeros_like(d, dtype=np.uint16)\n",
        "    lo, hi = np.percentile(d[d > 0], (1, 99))\n",
        "    d = np.clip(d, lo, hi)\n",
        "    d = (d - lo) / max(1e-6, (hi - lo))\n",
        "    d = np.clip(d, 1e-6, 1.0)  # avoid exact zeros\n",
        "    return (d * 65535.0).astype(np.uint16)\n",
        "\n",
        "def letterbox(img, target=768):\n",
        "    h, w = img.shape[:2]\n",
        "    s = target / max(h, w)\n",
        "    nh, nw = int(round(h*s)), int(round(w*s))\n",
        "    img_r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_CUBIC)\n",
        "    top = (target - nh)//2; bottom = target - nh - top\n",
        "    left = (target - nw)//2; right = target - nw - left\n",
        "    img_p = cv2.copyMakeBorder(img_r, top,bottom,left,right, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
        "    return img_p, (top,bottom,left,right), (h,w)\n",
        "\n",
        "def unletterbox(arr, pads, orig_hw):\n",
        "    top,bottom,left,right = pads\n",
        "    arr = arr[top:arr.shape[0]-bottom, left:arr.shape[1]-right]\n",
        "    return cv2.resize(arr, (orig_hw[1], orig_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# --- Collect images ---\n",
        "paths = sorted(p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
        "               for p in glob.glob(os.path.join(IMG_DIR, ext)))\n",
        "\n",
        "# --- Inference loop (prefer 768 input; fallback if not supported) ---\n",
        "import inspect\n",
        "sig = str(getattr(model, \"infer_image\", None))\n",
        "supports_size = \"input_size\" in (sig or \"\")\n",
        "\n",
        "for p in tqdm(paths):\n",
        "    img = cv2.imread(p)\n",
        "\n",
        "    if supports_size:\n",
        "        depth_rel = model.infer_image(img, input_size=768)  # HxW float (relative)\n",
        "    else:\n",
        "        # fallback: letterbox to 768, run, then unpad\n",
        "        img_p, pads, orig_hw = letterbox(img, 768)\n",
        "        depth_rel_p = model.infer_image(img_p)\n",
        "        depth_rel   = unletterbox(depth_rel_p, pads, orig_hw).astype(np.float32)\n",
        "\n",
        "    rel16 = to_u16_rel(depth_rel)\n",
        "    stem = os.path.splitext(os.path.basename(p))[0]\n",
        "    cv2.imwrite(f\"{OUT_REL}/{stem}.png\", rel16)\n",
        "    viz = cv2.applyColorMap(255 - (rel16 // 256).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
        "    cv2.imwrite(f\"{OUT_VIZ}/{stem}.jpg\", viz)\n",
        "\n",
        "print(\"Saved:\", len(paths), \"relative depth maps to\", OUT_REL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhG3haim_3NA",
        "outputId": "245b03e9-ecc7-42f9-b549-22d5254326e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:59<00:00,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: 18 relative depth maps to /content/drive/MyDrive/Depthanythingv2/output-vits/depths/raw_da2_vits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "059946aa"
      },
      "source": [
        "### Evaluation (vits model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "885c0adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "1220a89b-bde5-4a0c-ac4d-dea0adfd7787"
      },
      "source": [
        "import os, glob, cv2, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "IMG_DIR = \"/content/drive/MyDrive/Depthanythingv2/data/eval\"\n",
        "DEP_DIR = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/depths/raw_da2\"   # relative depth PNG16\n",
        "SEG_INST_DIR = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/seg\"        # optional (uint16 instance ids), can be None\n",
        "REPORT_CSV = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/depth_eval_report.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(REPORT_CSV), exist_ok=True)\n",
        "\n",
        "def read_depth_rel(path_png16):\n",
        "    d16 = cv2.imread(path_png16, -1).astype(np.float32)\n",
        "    d = d16 / 65535.0\n",
        "    d[~np.isfinite(d)] = np.nan\n",
        "    return d\n",
        "\n",
        "\n",
        "def sobel_grad(a):\n",
        "    a = np.asarray(a, dtype=np.float32)  # <-- ensure float32\n",
        "    gx = cv2.Sobel(a, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(a, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    return np.sqrt(gx*gx + gy*gy)\n",
        "\n",
        "def edge_alignment_metric(rgb, depth_rel):\n",
        "    # RGB -> edges\n",
        "    g_img = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY).astype(np.float32) / 255.0\n",
        "    g_img = cv2.GaussianBlur(g_img, (3,3), 0.8)\n",
        "    e_img = sobel_grad(g_img)\n",
        "\n",
        "    # Depth -> edges (normalize safely, keep float32)\n",
        "    x = depth_rel.copy()\n",
        "    x = x.astype(np.float32)               # <-- ensure float32\n",
        "    x[np.isnan(x)] = 0\n",
        "    if (x>0).sum() < 10:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    p1, p99 = np.percentile(x[x>0], (1,99))\n",
        "    x = (x - np.float32(p1)) / max(np.float32(1e-6), np.float32(p99 - p1))\n",
        "    x = np.clip(x, 0, 1).astype(np.float32)  # <-- ensure float32\n",
        "    e_depth = sobel_grad(x)\n",
        "\n",
        "    def binarize(e, pct=88):\n",
        "        th = np.percentile(e, pct)\n",
        "        return (e >= th).astype(np.uint8)\n",
        "\n",
        "    Eimg = binarize(e_img, 88)\n",
        "    Edep = binarize(e_depth, 88)\n",
        "\n",
        "    inter = (Eimg & Edep).sum()\n",
        "    dep_sum = Edep.sum() + 1e-6\n",
        "    img_sum = Eimg.sum() + 1e-6\n",
        "    prec = inter / dep_sum\n",
        "    rec  = inter / img_sum\n",
        "    f1   = 2*prec*rec / max(1e-6, (prec+rec))\n",
        "    return float(prec), float(rec), float(f1)\n",
        "\n",
        "\n",
        "def load_inst_mask(path):\n",
        "    if (path is None) or (not os.path.exists(path)):\n",
        "        return None\n",
        "    m = cv2.imread(path, -1)\n",
        "    if m is None:\n",
        "        return None\n",
        "    return m\n",
        "\n",
        "def ordinal_person_vs_bg(depth_rel, inst=None):\n",
        "    # Heuristic: person vs background ordering\n",
        "    if inst is None:\n",
        "        return np.nan\n",
        "    # assume person ids are >0; if you have class ids, you can refine later\n",
        "    # For now: treat the largest non-zero instance as foreground “person”-like\n",
        "    ids, counts = np.unique(inst[inst>0], return_counts=True)\n",
        "    if len(ids)==0:\n",
        "        return np.nan\n",
        "    fg_id = ids[np.argmax(counts)]\n",
        "    fg = (inst==fg_id)\n",
        "    if fg.sum() < 200:  # too tiny, skip\n",
        "        return np.nan\n",
        "    # background approx: farthest 20% depth pixels outside fg\n",
        "    bg = (~fg) & (depth_rel>0)\n",
        "    if bg.sum() < 200:\n",
        "        return np.nan\n",
        "    # sample N pairs\n",
        "    N = 2000\n",
        "    ys, xs = np.where(fg)\n",
        "    idx_fg = np.random.choice(len(xs), size=min(N, len(xs)), replace=False)\n",
        "    ys2, xs2 = np.where(bg)\n",
        "    idx_bg = np.random.choice(len(xs2), size=min(N, len(xs2)), replace=False)\n",
        "    df = depth_rel[ys[idx_fg], xs[idx_fg]]\n",
        "    db = depth_rel[ys2[idx_bg], xs2[idx_bg]]\n",
        "    # Nearer should have larger relative depth if we used inverse earlier,\n",
        "    # but here depth_rel is arbitrary. We'll check ratio ordering: fg should be \"closer\"\n",
        "    # Assume fg is closer => depth_rel[fg] < depth_rel[bg] (or > depending on model)\n",
        "    # Robust trick: compare ranks using median thresholds\n",
        "    med_fg = np.median(df[df>0]) if np.any(df>0) else 0\n",
        "    med_bg = np.median(db[db>0]) if np.any(db>0) else 0\n",
        "    # we don't know orientation; pick the orientation that gives higher accuracy\n",
        "    acc1 = np.mean(df < db)\n",
        "    acc2 = np.mean(df > db)\n",
        "    return float(max(acc1, acc2))\n",
        "\n",
        "def planarity_residual(depth_rel, rgb=None, inst=None):\n",
        "    # Fit a dominant plane on the largest smooth region (no GT needed).\n",
        "    # Normalize by median depth to make it scale-free.\n",
        "    H,W = depth_rel.shape\n",
        "    z = depth_rel.copy()\n",
        "    mask = z>0\n",
        "    if mask.sum() < 1000:\n",
        "        return np.nan\n",
        "    ys, xs = np.where(mask)\n",
        "    # Downsample points for speed\n",
        "    sel = np.random.choice(len(xs), size=min(20000, len(xs)), replace=False)\n",
        "    xs, ys = xs[sel], ys[sel]\n",
        "    zz = z[ys, xs]\n",
        "\n",
        "    # Backproject with a default K (consistent for relative comparisons)\n",
        "    f = 1.2*max(W,H); cx, cy = W/2, H/2\n",
        "    X = (xs - cx) * zz / f\n",
        "    Y = (ys - cy) * zz / f\n",
        "    pts = np.stack([X,Y,zz], axis=1)\n",
        "\n",
        "    # RANSAC plane fit\n",
        "    # Plane ax+by+cz+d=0; solve by SVD on random 3-pt samples; keep best inliers\n",
        "    best_rmse = None\n",
        "    rng = np.random.default_rng(0)\n",
        "    for _ in range(100):\n",
        "        i = rng.choice(len(pts), 3, replace=False)\n",
        "        P = pts[i]\n",
        "        v1 = P[1]-P[0]; v2 = P[2]-P[0]\n",
        "        n = np.cross(v1, v2)\n",
        "        if np.linalg.norm(n) < 1e-8:\n",
        "            continue\n",
        "        n = n / np.linalg.norm(n)\n",
        "        d = -np.dot(n, P[0])\n",
        "        # distance to plane\n",
        "        dist = np.abs(pts @ n + d)\n",
        "        rmse = np.sqrt(np.mean(dist**2))\n",
        "        if (best_rmse is None) or (rmse < best_rmse):\n",
        "            best_rmse = rmse\n",
        "    if best_rmse is None:\n",
        "        return np.nan\n",
        "    med_z = np.median(zz[zz>0]) if np.any(zz>0) else 1.0\n",
        "    return float(best_rmse / max(1e-6, med_z))  # scale-free residual\n",
        "\n",
        "def invalid_ratio(depth_rel):\n",
        "    return float(1.0 - (depth_rel>0).mean())\n",
        "\n",
        "rows=[]\n",
        "paths = sorted([p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
        "                for p in glob.glob(os.path.join(IMG_DIR, ext))])\n",
        "\n",
        "for p_rgb in tqdm(paths):\n",
        "    stem = os.path.splitext(os.path.basename(p_rgb))[0]\n",
        "    p_dep = os.path.join(DEP_DIR, f\"{stem}.png\")\n",
        "    if not os.path.exists(p_dep):\n",
        "        continue\n",
        "    rgb = cv2.imread(p_rgb)\n",
        "    depth = read_depth_rel(p_dep)\n",
        "\n",
        "    prec, rec, f1 = edge_alignment_metric(rgb, depth)\n",
        "\n",
        "    inst = None\n",
        "    p_inst = os.path.join(SEG_INST_DIR, f\"{stem}_inst.png\") if SEG_INST_DIR else None\n",
        "    if p_inst and os.path.exists(p_inst):\n",
        "        inst = load_inst_mask(p_inst)\n",
        "\n",
        "    ord_acc = ordinal_person_vs_bg(depth, inst)  # NaN if no masks\n",
        "\n",
        "    plan_res = planarity_residual(depth, rgb, inst)\n",
        "    inv_pct = invalid_ratio(depth)\n",
        "\n",
        "    rows.append([stem, prec, rec, f1, ord_acc, plan_res, inv_pct])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\n",
        "    \"img_id\",\"edge_prec\",\"edge_rec\",\"edge_f1\",\"ordinal_fg_bg_acc\",\"planarity_residual_rel\",\"invalid_ratio\"\n",
        "])\n",
        "\n",
        "# Simple overall score (higher is better):\n",
        "# Edge F1 (40%), Ordinal (30%), Planarity (20%, lower is better), Invalid (10%, lower is better)\n",
        "def score_row(r):\n",
        "    f1 = np.nan_to_num(r.edge_f1, nan=0.0)\n",
        "    ordv = np.nan_to_num(r.ordinal_fg_bg_acc, nan=0.5)  # if unknown, neutral\n",
        "    plan = np.nan_to_num(r.planarity_residual_rel, nan=0.1)\n",
        "    inv  = np.nan_to_num(r.invalid_ratio, nan=0.0)\n",
        "    # Convert “lower is better” to [0,1]\n",
        "    plan_ok = np.clip(1.0 - (plan/0.1), 0, 1)   # 0.1 ~ “ok” residual\n",
        "    inv_ok  = np.clip(1.0 - (inv/0.05), 0, 1)   # 5% invalid budget\n",
        "    return 0.4*f1 + 0.3*ordv + 0.2*plan_ok + 0.1*inv_ok\n",
        "\n",
        "df[\"overall_score\"] = df.apply(score_row, axis=1)\n",
        "df.to_csv(REPORT_CSV, index=False)\n",
        "df.sort_values(\"overall_score\", ascending=False).head(10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:06<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 img_id  edge_prec  edge_rec   edge_f1  \\\n",
              "3  e14c21ec-33d6-4aee-b254-9349418d5008   0.393004  0.393005  0.393004   \n",
              "1  c356067f-132c-4032-a812-4d8a405f6a92   0.289001  0.289001  0.289001   \n",
              "0  877eef99-d724-4928-93ca-8abfb07cf90a   0.292972  0.293027  0.292999   \n",
              "2  de1ad16a-c56d-45ed-ae2a-35da38c4f0aa   0.281538  0.281538  0.281538   \n",
              "\n",
              "   ordinal_fg_bg_acc  planarity_residual_rel  invalid_ratio  overall_score  \n",
              "3                NaN                0.218967       0.010059       0.387085  \n",
              "1                NaN                0.151955       0.019939       0.325722  \n",
              "0                NaN                0.139935       0.025712       0.315776  \n",
              "2                NaN                0.151061       0.025765       0.311085  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73e414f1-accd-4b6c-a604-2c2f550227f5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_id</th>\n",
              "      <th>edge_prec</th>\n",
              "      <th>edge_rec</th>\n",
              "      <th>edge_f1</th>\n",
              "      <th>ordinal_fg_bg_acc</th>\n",
              "      <th>planarity_residual_rel</th>\n",
              "      <th>invalid_ratio</th>\n",
              "      <th>overall_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>e14c21ec-33d6-4aee-b254-9349418d5008</td>\n",
              "      <td>0.393004</td>\n",
              "      <td>0.393005</td>\n",
              "      <td>0.393004</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.218967</td>\n",
              "      <td>0.010059</td>\n",
              "      <td>0.387085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>c356067f-132c-4032-a812-4d8a405f6a92</td>\n",
              "      <td>0.289001</td>\n",
              "      <td>0.289001</td>\n",
              "      <td>0.289001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.151955</td>\n",
              "      <td>0.019939</td>\n",
              "      <td>0.325722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>877eef99-d724-4928-93ca-8abfb07cf90a</td>\n",
              "      <td>0.292972</td>\n",
              "      <td>0.293027</td>\n",
              "      <td>0.292999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.139935</td>\n",
              "      <td>0.025712</td>\n",
              "      <td>0.315776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>de1ad16a-c56d-45ed-ae2a-35da38c4f0aa</td>\n",
              "      <td>0.281538</td>\n",
              "      <td>0.281538</td>\n",
              "      <td>0.281538</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.151061</td>\n",
              "      <td>0.025765</td>\n",
              "      <td>0.311085</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73e414f1-accd-4b6c-a604-2c2f550227f5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-73e414f1-accd-4b6c-a604-2c2f550227f5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-73e414f1-accd-4b6c-a604-2c2f550227f5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"img_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"c356067f-132c-4032-a812-4d8a405f6a92\",\n          \"de1ad16a-c56d-45ed-ae2a-35da38c4f0aa\",\n          \"e14c21ec-33d6-4aee-b254-9349418d5008\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edge_prec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05279695820391032,\n        \"min\": 0.2815376809139728,\n        \"max\": 0.3930043088571661,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.289000552160103,\n          0.2815376809139728,\n          0.3930043088571661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edge_rec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05278987029054298,\n        \"min\": 0.2815376809139728,\n        \"max\": 0.39300468242901454,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.289000552160103,\n          0.2815376809139728,\n          0.39300468242901454\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"edge_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05279341295379118,\n        \"min\": 0.2815376809139728,\n        \"max\": 0.3930044956430015,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.289000552160103,\n          0.2815376809139728,\n          0.3930044956430015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ordinal_fg_bg_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"planarity_residual_rel\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03607526729833916,\n        \"min\": 0.13993470394266697,\n        \"max\": 0.2189671892023535,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"invalid_ratio\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007397216234813724,\n        \"min\": 0.010058504746880526,\n        \"max\": 0.025765044071840437,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"overall_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.035310000239907824,\n        \"min\": 0.3110849842219082,\n        \"max\": 0.38708478876343955,\n        \"num_unique_values\": 4,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Depthanythingv2/output-vits/depth_eval_report.csv\")\n",
        "mean = df[\"overall_score\"].mean()\n",
        "std = df[\"overall_score\"].std()\n",
        "print(f\"Average overall_score = {mean:.3f} ± {std:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E96XR0ga4zq",
        "outputId": "76feee18-cc2f-4d38-f933-71597e71f92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average overall_score = 0.331 ± 0.040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eUXzHct6gN1"
      },
      "source": [
        "### Code (vitb model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF5shH126gN1"
      },
      "source": [
        "IMG_DIR  = \"/content/drive/MyDrive/Depthanythingv2/data/eval\"  # your test images\n",
        "REPO     = \"/content/Depth-Anything-V2\"\n",
        "CKPT     = f\"{REPO}/checkpoints/depth_anything_v2_vitb.pth\"     # vitb weights\n",
        "OUT_REL  = \"/content/drive/MyDrive/Depthanythingv2/output-vitb/depths/raw_da2_vitb896_tta\"\n",
        "OUT_VIZ  = \"/content/drive/MyDrive/Depthanythingv2/output-vitb/depths/viz_da2_vitb896_tta\"\n",
        "\n",
        "import os, sys\n",
        "os.makedirs(OUT_REL, exist_ok=True); os.makedirs(OUT_VIZ, exist_ok=True)\n",
        "sys.path.append(REPO)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.backends.cudnn.benchmark = True  # speed on fixed input shapes\n",
        "\n",
        "model = DepthAnythingV2(\n",
        "    encoder='vitb',\n",
        "    features=128,\n",
        "    out_channels=[96, 192, 384, 768]\n",
        ")\n",
        "state = torch.load(CKPT, map_location='cpu')\n",
        "model.load_state_dict(state, strict=True)\n",
        "model = model.to(DEVICE).eval()\n",
        "\n",
        "print(\"Loaded vitb on\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "8be320a8-3ce2-4a41-f5dc-5b03f0c51e92",
        "id": "B9-7C-WR6gN1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Depth-Anything-V2/checkpoints/depth_anything_v2_vitb.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1832399625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m192\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCKPT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Depth-Anything-V2/checkpoints/depth_anything_v2_vitb.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, glob, numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def letterbox(img, target=896):\n",
        "    h, w = img.shape[:2]\n",
        "    s = target / max(h, w)\n",
        "    nh, nw = int(round(h*s)), int(round(w*s))\n",
        "    img_r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_CUBIC)\n",
        "    top = (target - nh)//2; bottom = target - nh - top\n",
        "    left = (target - nw)//2; right = target - nw - left\n",
        "    img_p = cv2.copyMakeBorder(img_r, top,bottom,left,right, cv2.BORDER_CONSTANT, value=(0,0,0))\n",
        "    return img_p, (top,bottom,left,right), (h,w)\n",
        "\n",
        "def unletterbox(arr, pads, orig_hw):\n",
        "    top,bottom,left,right = pads\n",
        "    arr = arr[top:arr.shape[0]-bottom, left:arr.shape[1]-right]\n",
        "    return cv2.resize(arr, (orig_hw[1], orig_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_tta_rel(img_bgr):\n",
        "    \"\"\"Return relative depth (float32) using flip-TTA + AMP when on CUDA.\"\"\"\n",
        "    if DEVICE == 'cuda':\n",
        "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "            d0 = model.infer_image(img_bgr).astype(np.float32)\n",
        "            d1 = model.infer_image(cv2.flip(img_bgr, 1)).astype(np.float32)\n",
        "    else:\n",
        "        d0 = model.infer_image(img_bgr).astype(np.float32)\n",
        "        d1 = model.infer_image(cv2.flip(img_bgr, 1)).astype(np.float32)\n",
        "    d1 = np.flip(d1, axis=1)\n",
        "    return (0.5*(d0 + d1)).astype(np.float32)\n",
        "\n",
        "def to_u16_rel(depth):\n",
        "    \"\"\"Robust 16-bit scaling for relative depth; avoids exact zeros.\"\"\"\n",
        "    d = depth.astype(np.float32)\n",
        "    d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    if (d>0).sum() < 10:\n",
        "        return np.zeros_like(d, np.uint16)\n",
        "    p1, p99 = np.percentile(d[d>0], (1,99))\n",
        "    p1 = np.float32(p1); p99 = np.float32(p99)\n",
        "    d = np.clip(d, p1, p99)\n",
        "    d = (d - p1) / max(np.float32(1e-6), (p99 - p1))\n",
        "    d = np.clip(d, np.float32(1e-6), np.float32(1.0))\n",
        "    return (d * np.float32(65535.0)).astype(np.uint16)\n"
      ],
      "metadata": {
        "id": "bbLSdmUq6gN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = sorted(p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
        "               for p in glob.glob(os.path.join(IMG_DIR, ext)))\n",
        "\n",
        "for p in tqdm(paths):\n",
        "    img = cv2.imread(p)\n",
        "    img_p, pads, orig_hw = letterbox(img, 896)\n",
        "    d_rel_p = infer_tta_rel(img_p)                 # float32\n",
        "    d_rel   = unletterbox(d_rel_p, pads, orig_hw)  # back to original size\n",
        "\n",
        "    rel16 = to_u16_rel(d_rel)\n",
        "    stem = os.path.splitext(os.path.basename(p))[0]\n",
        "    cv2.imwrite(f\"{OUT_REL}/{stem}.png\", rel16)\n",
        "\n",
        "    viz = cv2.applyColorMap(255 - (rel16//256).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
        "    cv2.imwrite(f\"{OUT_VIZ}/{stem}.jpg\", viz)\n",
        "\n",
        "print(\"Saved:\", len(paths), \"depth maps to\", OUT_REL)\n"
      ],
      "metadata": {
        "id": "YHa2KTr76gN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQWmycFa6gN2"
      },
      "source": [
        "### Evaluation (vitb model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8y6rKLm6gN2"
      },
      "source": [
        "import os, glob, cv2, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "IMG_DIR = IMG_DIR\n",
        "DEP_DIR = OUT_REL\n",
        "REPORT  = \"/content/drive/MyDrive/Depthanythingv2/output-vitb/depth_eval_report_vitb896_tta.csv\"\n",
        "os.makedirs(os.path.dirname(REPORT), exist_ok=True)\n",
        "\n",
        "def read_depth_rel(png16):\n",
        "    d16 = cv2.imread(png16, -1).astype(np.float32)\n",
        "    d = d16 / np.float32(65535.0)\n",
        "    d[~np.isfinite(d)] = np.nan\n",
        "    return d.astype(np.float32)\n",
        "\n",
        "def sobel_grad(a):\n",
        "    a = np.asarray(a, dtype=np.float32)\n",
        "    gx = cv2.Sobel(a, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(a, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    return np.sqrt(gx*gx + gy*gy).astype(np.float32)\n",
        "\n",
        "def edge_metrics(rgb, depth_rel):\n",
        "    g = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY).astype(np.float32)/255.0\n",
        "    g = cv2.GaussianBlur(g,(3,3),0.8).astype(np.float32)\n",
        "    e_img = sobel_grad(g)\n",
        "\n",
        "    x = depth_rel.astype(np.float32); x[np.isnan(x)] = 0.0\n",
        "    if (x>0).sum()<10: return 0.0,0.0,0.0\n",
        "    p1,p99 = np.percentile(x[x>0],(1,99)); p1=np.float32(p1); p99=np.float32(p99)\n",
        "    x = np.clip((x-p1)/max(np.float32(1e-6), (p99-p1)), 0, 1).astype(np.float32)\n",
        "    e_dep = sobel_grad(x)\n",
        "\n",
        "    t1 = np.float32(np.percentile(e_img,88))\n",
        "    t2 = np.float32(np.percentile(e_dep,88))\n",
        "    E1 = (e_img>=t1).astype(np.uint8); E2 = (e_dep>=t2).astype(np.uint8)\n",
        "\n",
        "    inter = (E1 & E2).sum(); dep_sum = E2.sum()+1e-6; img_sum = E1.sum()+1e-6\n",
        "    prec = inter/dep_sum; rec = inter/img_sum\n",
        "    f1 = 2*prec*rec/max(1e-6,(prec+rec))\n",
        "    return float(prec), float(rec), float(f1)\n",
        "\n",
        "def planarity(depth_rel):\n",
        "    z = depth_rel.astype(np.float32)\n",
        "    mask = np.isfinite(z) & (z>0)\n",
        "    if mask.sum() < 1000: return np.nan\n",
        "    ys,xs = np.where(mask)\n",
        "    sel = np.random.choice(len(xs), size=min(20000,len(xs)), replace=False)\n",
        "    xs,ys = xs[sel],ys[sel]; zz = z[ys,xs].astype(np.float32)\n",
        "\n",
        "    H,W = z.shape; f=np.float32(1.2*max(W,H)); cx=np.float32(W/2); cy=np.float32(H/2)\n",
        "    X=(xs.astype(np.float32)-cx)*zz/f; Y=(ys.astype(np.float32)-cy)*zz/f\n",
        "    P=np.stack([X,Y,zz],1).astype(np.float32)\n",
        "\n",
        "    best=None; rng=np.random.default_rng(0)\n",
        "    for _ in range(100):\n",
        "        i=rng.choice(len(P),3,replace=False)\n",
        "        v1=P[i[1]]-P[i[0]]; v2=P[i[2]]-P[i[0]]\n",
        "        n=np.cross(v1,v2).astype(np.float32); n_norm=np.linalg.norm(n)\n",
        "        if n_norm<1e-8: continue\n",
        "        n/=n_norm; d0=-np.dot(n,P[i[0]]).astype(np.float32)\n",
        "        dist=np.abs(P@n + d0).astype(np.float32)\n",
        "        rmse=float(np.sqrt(np.mean(dist**2)))\n",
        "        if (best is None) or (rmse<best): best=rmse\n",
        "    if best is None: return np.nan\n",
        "    med=float(np.median(zz[zz>0])) if np.any(zz>0) else 1.0\n",
        "    return float(best/max(1e-6, med))\n",
        "\n",
        "def invalid_ratio(depth_rel):\n",
        "    return float(np.isnan(depth_rel).mean())\n",
        "\n",
        "rows=[]\n",
        "img_paths = sorted(p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
        "                   for p in glob.glob(os.path.join(IMG_DIR, ext)))\n",
        "for p_rgb in tqdm(img_paths):\n",
        "    stem = os.path.splitext(os.path.basename(p_rgb))[0]\n",
        "    p_dep = os.path.join(DEP_DIR, f\"{stem}.png\")\n",
        "    if not os.path.exists(p_dep): continue\n",
        "    rgb = cv2.imread(p_rgb); depth = read_depth_rel(p_dep)\n",
        "    prec,rec,f1 = edge_metrics(rgb, depth)\n",
        "    plan = planarity(depth); inv = invalid_ratio(depth)\n",
        "    rows.append([stem,prec,rec,f1,plan,inv])\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "df = pd.DataFrame(rows, columns=[\"img_id\",\"edge_prec\",\"edge_rec\",\"edge_f1\",\"planarity_residual_rel\",\"invalid_ratio\"])\n",
        "if df.empty:\n",
        "    raise RuntimeError(\"No matches: check that PNGs in OUT_REL share basenames with images.\")\n",
        "\n",
        "f1   = df[\"edge_f1\"].astype(float).fillna(0.0)\n",
        "plan = df[\"planarity_residual_rel\"].astype(float).fillna(0.1)\n",
        "inv  = df[\"invalid_ratio\"].astype(float).fillna(0.0)\n",
        "plan_ok = np.clip(1.0 - (plan/0.1), 0.0, 1.0)\n",
        "inv_ok  = np.clip(1.0 - (inv /0.05), 0.0, 1.0)\n",
        "df[\"overall_score\"] = 0.6*f1 + 0.25*plan_ok + 0.15*inv_ok\n",
        "\n",
        "df.to_csv(REPORT, index=False)\n",
        "print(f\"Average overall_score = {df['overall_score'].mean():.3f} ± {df['overall_score'].std():.3f}\")\n",
        "df.sort_values(\"overall_score\", ascending=False).head(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Depthanythingv2/output-vitb/depth_eval_report.csv\")\n",
        "mean = df[\"overall_score\"].mean()\n",
        "std = df[\"overall_score\"].std()\n",
        "print(f\"Average overall_score = {mean:.3f} ± {std:.3f}\")"
      ],
      "metadata": {
        "id": "bhu7zn04g4u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtYwKUBn6dmM"
      },
      "source": [
        "### Code (vitl model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os, glob\n",
        "\n",
        "REPO = \"/content/drive/MyDrive/Depthanythingv2\"\n",
        "dirs = glob.glob(REPO + \"/**/depth_anything_v2\", recursive=True)\n",
        "\n",
        "if not dirs:\n",
        "    raise RuntimeError(\"Can't find depth_anything_v2 folder anywhere under REPO.\")\n",
        "\n",
        "module_dir = dirs[0]                      # .../depth_anything_v2\n",
        "repo_root = os.path.dirname(module_dir)   # the folder that CONTAINS depth_anything_v2\n",
        "\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "\n",
        "print(\"Using repo_root:\", repo_root)\n",
        "print(\"sys.path[0]:\", sys.path[0])\n",
        "\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "print(\"OK import DepthAnythingV2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_rawq4PnHug",
        "outputId": "d24b9d2f-5464-44b9-ceeb-8e2a3ff56146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using repo_root: /content/drive/MyDrive/Depthanythingv2/Depth-Anything-V2\n",
            "sys.path[0]: /content/drive/MyDrive/Depthanythingv2/Depth-Anything-V2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:dinov2:xFormers not available\n",
            "WARNING:dinov2:xFormers not available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK import DepthAnythingV2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5itcVVy6dmM"
      },
      "source": [
        "import os, glob, sys, cv2, numpy as np, torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Your image folder on Drive\n",
        "IMG_DIR = \"/content/drive/MyDrive/Segmentation/dataset\"\n",
        "\n",
        "# Output folders (new)\n",
        "OUT_REL = \"/content/drive/MyDrive/Segmentation/depth-maps/raw_da2_vitl896_tta\"\n",
        "OUT_VIZ = \"/content/drive/MyDrive/Segmentation/depth-maps/viz_da2_vitl896_tta\"\n",
        "\n",
        "# DA2 repo path + checkpoint path\n",
        "REPO = \"/content/drive/MyDrive/Depthanythingv2\"\n",
        "CKPT = \"/content/drive/MyDrive/Depthanythingv2/checkpoints/depth_anything_v2_vitl.pth\"\n",
        "\n",
        "os.makedirs(OUT_REL, exist_ok=True)\n",
        "os.makedirs(OUT_VIZ, exist_ok=True)\n",
        "\n",
        "# Make sure we can import the library\n",
        "sys.path.append(REPO)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_configs = {\n",
        "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]}\n",
        "}\n",
        "\n",
        "model = DepthAnythingV2(**model_configs['vitl'])\n",
        "state = torch.load(CKPT, map_location='cpu')\n",
        "model.load_state_dict(state, strict=True)\n",
        "model = model.to(DEVICE).eval()\n",
        "\n",
        "print(\"Loaded vitl on\", DEVICE)\n"
      ],
      "metadata": {
        "id": "r-tHUQdw6dmM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86eddd8-efce-45a0-8ad7-c85821f01a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded vitl on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def letterbox(img, target=896):\n",
        "    h, w = img.shape[:2]\n",
        "    s = target / max(h, w)\n",
        "    nh, nw = int(round(h * s)), int(round(w * s))\n",
        "    img_r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_CUBIC)\n",
        "    top = (target - nh) // 2; bottom = target - nh - top\n",
        "    left = (target - nw) // 2; right = target - nw - left\n",
        "    img_p = cv2.copyMakeBorder(img_r, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "    return img_p, (top, bottom, left, right), (h, w)\n",
        "\n",
        "def unletterbox(arr, pads, orig_hw):\n",
        "    top, bottom, left, right = pads\n",
        "    arr = arr[top:arr.shape[0]-bottom, left:arr.shape[1]-right]\n",
        "    return cv2.resize(arr, (orig_hw[1], orig_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "def infer_tta_rel(img_bgr):\n",
        "    # Predict and average normal + flipped; keep float32\n",
        "    d0 = model.infer_image(img_bgr).astype(np.float32)\n",
        "    d1 = model.infer_image(cv2.flip(img_bgr, 1)).astype(np.float32)\n",
        "    d1 = np.flip(d1, axis=1)\n",
        "    d = 0.5 * (d0 + d1)\n",
        "    return d.astype(np.float32)\n",
        "\n",
        "def to_u16_rel(depth):\n",
        "    d = depth.astype(np.float32)\n",
        "    d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    if (d > 0).sum() < 10:\n",
        "        return np.zeros_like(d, dtype=np.uint16)\n",
        "    p1, p99 = np.percentile(d[d > 0], (1, 99))\n",
        "    p1 = np.float32(p1); p99 = np.float32(p99)\n",
        "    d = np.clip(d, p1, p99)\n",
        "    d = (d - p1) / max(np.float32(1e-6), (p99 - p1))\n",
        "    d = np.clip(d, np.float32(1e-6), np.float32(1.0))   # avoid exact zeros\n",
        "    return (d * np.float32(65535.0)).astype(np.uint16)\n"
      ],
      "metadata": {
        "id": "4-nmCeKl6dmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = sorted(\n",
        "    p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
        "    for p in glob.glob(os.path.join(IMG_DIR, ext))\n",
        ")\n",
        "\n",
        "for p in tqdm(paths):\n",
        "    img = cv2.imread(p)                    # uint8\n",
        "    img_p, pads, orig_hw = letterbox(img, 896)\n",
        "    depth_rel_p = infer_tta_rel(img_p)     # float32\n",
        "    depth_rel = unletterbox(depth_rel_p, pads, orig_hw).astype(np.float32)\n",
        "\n",
        "    rel16 = to_u16_rel(depth_rel)\n",
        "    stem = os.path.splitext(os.path.basename(p))[0]\n",
        "    cv2.imwrite(f\"{OUT_REL}/{stem}.png\", rel16)\n",
        "\n",
        "    viz = cv2.applyColorMap(255 - (rel16 // 256).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
        "    cv2.imwrite(f\"{OUT_VIZ}/{stem}.jpg\", viz)\n",
        "\n",
        "print(\"Saved\", len(paths), \"depth maps to\", OUT_REL)\n"
      ],
      "metadata": {
        "id": "xc09lbPT6dmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865a7d84-276e-442b-9338-5ba51afd8fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 563/563 [11:22:28<00:00, 72.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 563 depth maps to /content/drive/MyDrive/Segmentation/depth-maps/raw_da2_vitl896_tta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls  /content/drive/MyDrive/Depthanythingv2/output-vitl/depths/raw_da2_vitl896_tta"
      ],
      "metadata": {
        "id": "QkxUzJEtlwrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, sys, cv2, numpy as np, torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==== paths (same as yours) ====\n",
        "IMG_DIR = \"/content/drive/MyDrive/Depthanythingv2/data/eval\"\n",
        "OUT_REL = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/depths/raw_da2_vitl896_1152_ms_tta4_jbf\"\n",
        "OUT_VIZ = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/depths/viz_da2_vitl896_1152_ms_tta4_jbf\"\n",
        "REPO    = \"/content/drive/MyDrive/Depthanythingv2\"\n",
        "CKPT    = f\"{REPO}/checkpoints/depth_anything_v2_vitl.pth\"\n",
        "os.makedirs(OUT_REL, exist_ok=True); os.makedirs(OUT_VIZ, exist_ok=True)\n",
        "sys.path.append(REPO)\n",
        "\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = DepthAnythingV2(encoder='vitl', features=256, out_channels=[256,512,1024,1024])\n",
        "state = torch.load(CKPT, map_location='cpu'); model.load_state_dict(state, strict=True)\n",
        "model = model.to(DEVICE).eval()\n",
        "print(\"Loaded vitl on\", DEVICE)\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def letterbox_reflect(img, target):\n",
        "    h, w = img.shape[:2]\n",
        "    s = target / max(h, w)\n",
        "    nh, nw = int(round(h * s)), int(round(w * s))\n",
        "    img_r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA if s < 1.0 else cv2.INTER_CUBIC)\n",
        "    top = (target - nh) // 2; bottom = target - nh - top\n",
        "    left = (target - nw) // 2; right = target - nw - left\n",
        "    img_p = cv2.copyMakeBorder(img_r, top, bottom, left, right, cv2.BORDER_REFLECT_101)\n",
        "    return img_p, (top, bottom, left, right), (h, w)\n",
        "\n",
        "def unletterbox(arr, pads, orig_hw):\n",
        "    top, bottom, left, right = pads\n",
        "    arr = arr[top:arr.shape[0]-bottom, left:arr.shape[1]-right]\n",
        "    return cv2.resize(arr, (orig_hw[1], orig_hw[0]), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_one(img_bgr):\n",
        "    # model.infer_image accepts uint8 BGR; returns float32 depth (relative)\n",
        "    return model.infer_image(img_bgr).astype(np.float32)\n",
        "\n",
        "def infer_tta4(img_bgr):\n",
        "    # none, h, v, hv; unflip back and average\n",
        "    d0 = infer_one(img_bgr)\n",
        "    d1 = np.flip(infer_one(cv2.flip(img_bgr, 1)), axis=1)  # h\n",
        "    d2 = np.flip(infer_one(cv2.flip(img_bgr, 0)), axis=0)  # v\n",
        "    d3 = np.flip(np.flip(infer_one(cv2.flip(cv2.flip(img_bgr, 1), 0)), axis=1), axis=0)  # hv\n",
        "    return (d0 + d1 + d2 + d3) / 4.0\n",
        "\n",
        "def infer_ms_tta(img_bgr, sizes=(896, 1152)):\n",
        "    outs = []\n",
        "    for s in sizes:\n",
        "        img_p, pads, orig_hw = letterbox_reflect(img_bgr, s)\n",
        "        d = infer_tta4(img_p)\n",
        "        d = unletterbox(d, pads, orig_hw).astype(np.float32)\n",
        "        outs.append(d)\n",
        "    # resize all to original (already done) and average\n",
        "    return np.mean(outs, axis=0).astype(np.float32)\n",
        "\n",
        "def joint_bilateral_depth(depth, guide_bgr, ds=7, dr=0.1, iters=2):\n",
        "    # depth in [0,1] relative; guide is BGR uint8. Use domain transform-like iterative bilateral.\n",
        "    d = depth.copy().astype(np.float32)\n",
        "    g = guide_bgr\n",
        "    for _ in range(iters):\n",
        "        # OpenCV doesn't have true joint bilateral; use bilateral on depth plus small guidance mix\n",
        "        # Build an edge map to preserve discontinuities\n",
        "        edges = cv2.Canny(cv2.cvtColor(g, cv2.COLOR_BGR2GRAY), 50, 150).astype(np.float32)/255.0\n",
        "        # Light bilateral\n",
        "        d_blur = cv2.bilateralFilter(d, ds, dr*255.0, ds)\n",
        "        # Keep strong edges from original, smooth elsewhere\n",
        "        w = cv2.GaussianBlur(edges, (0,0), 1.0)\n",
        "        w = np.clip(1.0 - w, 0.0, 1.0).astype(np.float32)\n",
        "        d = w*d_blur + (1.0 - w)*d\n",
        "    return d\n",
        "\n",
        "def to_u16_rel(depth):\n",
        "    d = depth.astype(np.float32)\n",
        "    d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    if (d > 0).sum() < 10:\n",
        "        return np.zeros_like(d, dtype=np.uint16)\n",
        "    p_lo, p_hi = np.percentile(d[d > 0], (0.5, 99.5))\n",
        "    p_lo = np.float32(p_lo); p_hi = np.float32(p_hi)\n",
        "    d = np.clip((d - p_lo) / max(np.float32(1e-6), (p_hi - p_lo)), 0, 1)\n",
        "    return (d * np.float32(65535.0)).astype(np.uint16)\n",
        "\n",
        "# ---------- run ----------\n",
        "paths = sorted(p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
        "               for p in glob.glob(os.path.join(IMG_DIR, ext)))\n",
        "\n",
        "for p in tqdm(paths):\n",
        "    img = cv2.imread(p)\n",
        "    depth_rel = infer_ms_tta(img, sizes=(896,1152))  # <- multi-scale + TTA(4)\n",
        "\n",
        "    # edge-preserving refinement guided by RGB to sharpen boundaries / flatten planes\n",
        "    x = depth_rel.copy()\n",
        "    # bring to [0,1] before refinement to stabilize bilateral behavior\n",
        "    if (x>0).sum() >= 10:\n",
        "        q1, q99 = np.percentile(x[x>0], (1,99))\n",
        "        x = np.clip((x - q1) / max(1e-6, (q99 - q1)), 0, 1).astype(np.float32)\n",
        "    x = joint_bilateral_depth(x, img, ds=7, dr=0.08, iters=2)\n",
        "    depth_rel = x.astype(np.float32)\n",
        "\n",
        "    stem = os.path.splitext(os.path.basename(p))[0]\n",
        "    rel16 = to_u16_rel(depth_rel)\n",
        "    cv2.imwrite(f\"{OUT_REL}/{stem}.png\", rel16)\n",
        "\n",
        "    viz = cv2.applyColorMap(255 - (rel16 // 256).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
        "    cv2.imwrite(f\"{OUT_VIZ}/{stem}.jpg\", viz)\n",
        "\n",
        "print(\"Saved\", len(paths), \"depth maps to\", OUT_REL)\n"
      ],
      "metadata": {
        "id": "n9mfWIY5nA9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oRaIxxs6dmN"
      },
      "source": [
        "### Evaluation (vitl model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF30aKP56dmN"
      },
      "source": [
        "import os, glob, cv2, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- paths: point DEP_DIR to your vitl outputs ---\n",
        "IMG_DIR    = \"/content/drive/MyDrive/Depthanythingv2/data/eval\"\n",
        "DEP_DIR    = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/depths/raw_da2_vitl896_1152_ms_tta4_jbf\"\n",
        "REPORT_CSV = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/depth_eval_report_vitl896_1152_ms_tta4_jbf.csv\"\n",
        "os.makedirs(os.path.dirname(REPORT_CSV), exist_ok=True)\n",
        "\n",
        "# --- helpers ---\n",
        "def read_depth_rel(path_png16):\n",
        "    d16 = cv2.imread(path_png16, -1).astype(np.float32)\n",
        "    d = d16 / np.float32(65535.0)\n",
        "    d[~np.isfinite(d)] = np.nan\n",
        "    return d.astype(np.float32)\n",
        "\n",
        "def sobel_grad(a):\n",
        "    a = np.asarray(a, dtype=np.float32)\n",
        "    gx = cv2.Sobel(a, cv2.CV_32F, 1, 0, ksize=3)\n",
        "    gy = cv2.Sobel(a, cv2.CV_32F, 0, 1, ksize=3)\n",
        "    return np.sqrt(gx*gx + gy*gy).astype(np.float32)\n",
        "\n",
        "def edge_metrics(rgb, depth_rel):\n",
        "    g = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY).astype(np.float32)/255.0\n",
        "    g = cv2.GaussianBlur(g,(3,3),0.8).astype(np.float32)\n",
        "    e_img = sobel_grad(g)\n",
        "\n",
        "    x = depth_rel.astype(np.float32)\n",
        "    x[np.isnan(x)] = 0.0\n",
        "    if (x>0).sum() < 10:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    p1,p99 = np.percentile(x[x>0], (1,99))\n",
        "    p1 = np.float32(p1); p99 = np.float32(p99)\n",
        "    x = np.clip((x - p1) / max(np.float32(1e-6), (p99 - p1)), 0, 1).astype(np.float32)\n",
        "    e_dep = sobel_grad(x)\n",
        "\n",
        "    t1 = np.float32(np.percentile(e_img, 88))\n",
        "    t2 = np.float32(np.percentile(e_dep, 88))\n",
        "    E1 = (e_img >= t1).astype(np.uint8)\n",
        "    E2 = (e_dep >= t2).astype(np.uint8)\n",
        "\n",
        "    inter = (E1 & E2).sum()\n",
        "    prec  = inter / (E2.sum() + 1e-6)\n",
        "    rec   = inter / (E1.sum() + 1e-6)\n",
        "    f1    = 2*prec*rec / max(1e-6, (prec+rec))\n",
        "    return float(prec), float(rec), float(f1)\n",
        "\n",
        "def planarity(depth_rel):\n",
        "    z = depth_rel.astype(np.float32)\n",
        "    mask = np.isfinite(z) & (z>0)\n",
        "    if mask.sum() < 1000:\n",
        "        return np.nan\n",
        "    ys,xs = np.where(mask)\n",
        "    sel = np.random.choice(len(xs), size=min(20000,len(xs)), replace=False)\n",
        "    xs,ys = xs[sel],ys[sel]; zz = z[ys,xs].astype(np.float32)\n",
        "\n",
        "    H,W = z.shape; f = np.float32(1.2*max(W,H)); cx = np.float32(W/2); cy = np.float32(H/2)\n",
        "    X = (xs.astype(np.float32) - cx)*zz/f\n",
        "    Y = (ys.astype(np.float32) - cy)*zz/f\n",
        "    P = np.stack([X,Y,zz],1).astype(np.float32)\n",
        "\n",
        "    best=None; rng = np.random.default_rng(0)\n",
        "    for _ in range(100):\n",
        "        i = rng.choice(len(P), 3, replace=False)\n",
        "        v1 = P[i[1]] - P[i[0]]; v2 = P[i[2]] - P[i[0]]\n",
        "        n  = np.cross(v1, v2).astype(np.float32)\n",
        "        n_norm = np.linalg.norm(n)\n",
        "        if n_norm < 1e-8: continue\n",
        "        n /= n_norm; d0 = -np.dot(n, P[i[0]]).astype(np.float32)\n",
        "        dist = np.abs(P @ n + d0).astype(np.float32)\n",
        "        rmse = float(np.sqrt(np.mean(dist**2)))\n",
        "        if (best is None) or (rmse < best): best = rmse\n",
        "    if best is None: return np.nan\n",
        "    med = float(np.median(zz[zz>0])) if np.any(zz>0) else 1.0\n",
        "    return float(best / max(1e-6, med))\n",
        "\n",
        "def invalid_ratio(depth_rel):\n",
        "    return float(np.isnan(depth_rel).mean())\n",
        "\n",
        "# --- compute rows ---\n",
        "rows = []\n",
        "img_paths = sorted(p for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.JPG\",\"*.PNG\")\n",
        "                   for p in glob.glob(os.path.join(IMG_DIR, ext)))\n",
        "\n",
        "for p_rgb in tqdm(img_paths):\n",
        "    stem = os.path.splitext(os.path.basename(p_rgb))[0]\n",
        "    p_dep = os.path.join(DEP_DIR, f\"{stem}.png\")\n",
        "    if not os.path.exists(p_dep):\n",
        "        continue\n",
        "    rgb   = cv2.imread(p_rgb)\n",
        "    depth = read_depth_rel(p_dep)\n",
        "    prec, rec, f1 = edge_metrics(rgb, depth)\n",
        "    plan = planarity(depth)\n",
        "    inv  = invalid_ratio(depth)\n",
        "    rows.append([stem, prec, rec, f1, plan, inv])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\n",
        "    \"img_id\",\"edge_prec\",\"edge_rec\",\"edge_f1\",\"planarity_residual_rel\",\"invalid_ratio\"\n",
        "])\n",
        "\n",
        "if df.empty:\n",
        "    raise RuntimeError(f\"No matches found. Check DEP_DIR: {DEP_DIR}\")\n",
        "\n",
        "# --- vectorized overall score ---\n",
        "f1   = df[\"edge_f1\"].astype(float).fillna(0.0)\n",
        "plan = df[\"planarity_residual_rel\"].astype(float).fillna(0.1)\n",
        "inv  = df[\"invalid_ratio\"].astype(float).fillna(0.0)\n",
        "plan_ok = np.clip(1.0 - (plan/0.1), 0.0, 1.0)\n",
        "inv_ok  = np.clip(1.0 - (inv /0.05), 0.0, 1.0)\n",
        "df[\"overall_score\"] = 0.6*f1 + 0.25*plan_ok + 0.15*inv_ok\n",
        "\n",
        "df.to_csv(REPORT_CSV, index=False)\n",
        "print(f\"Average overall_score = {df['overall_score'].mean():.3f} ± {df['overall_score'].std():.3f}\")\n",
        "df.sort_values(\"overall_score\", ascending=False).head(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Depthanythingv2/output-vitl/depth_eval_report_vitl896_tta.csv\")\n",
        "mean = df[\"overall_score\"].mean()\n",
        "std = df[\"overall_score\"].std()\n",
        "print(f\"Average overall_score = {mean:.3f} ± {std:.3f}\")"
      ],
      "metadata": {
        "id": "TayQ_NLSpzI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sz-JHEWV2uZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt_Vz21F24KQ"
      },
      "source": [
        "### visual"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- EDIT THESE IF YOUR FILES ARE ELSEWHERE ----\n",
        "P_VITS = \"/content/drive/MyDrive/Depthanythingv2/output-vits/depth_eval_report.csv\"\n",
        "P_VITB = \"/content/drive/MyDrive/Depthanythingv2/output-vitb/depth_eval_report_vitb896_tta.csv\"\n",
        "P_VITL = \"/content/drive/MyDrive/Depthanythingv2/output-vitl/depth_eval_report_vitl896_tta.csv\"\n",
        "# ------------------------------------------------\n",
        "\n",
        "import os, pandas as pd, numpy as np\n",
        "\n",
        "def load_eval(path, model_tag):\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    df = pd.read_csv(path)\n",
        "    need = {\"img_id\",\"edge_prec\",\"edge_rec\",\"edge_f1\",\"planarity_residual_rel\",\"invalid_ratio\",\"overall_score\"}\n",
        "    if not need.issubset(df.columns):\n",
        "        return None\n",
        "    df = df.copy()\n",
        "    df[\"model\"] = model_tag\n",
        "    return df\n",
        "\n",
        "dfs = []\n",
        "for p, tag in [(P_VITS,\"vits\"), (P_VITB,\"vitb\"), (P_VITL,\"vitl\")]:\n",
        "    d = load_eval(p, tag)\n",
        "    if d is not None:\n",
        "        dfs.append(d)\n",
        "\n",
        "if not dfs:\n",
        "    raise RuntimeError(\"No eval CSVs loaded. Double-check the three P_* paths at the top.\")\n",
        "\n",
        "all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Summary per model\n",
        "summary = (\n",
        "    all_df.groupby(\"model\")[[\"overall_score\",\"edge_f1\",\"planarity_residual_rel\",\"invalid_ratio\"]]\n",
        "    .agg([\"count\",\"mean\",\"std\"])\n",
        ")\n",
        "\n",
        "# Per-image wide table for overall_score (handy to compare same image across models)\n",
        "wide_overall = all_df.pivot_table(index=\"img_id\", columns=\"model\", values=\"overall_score\", aggfunc=\"mean\")\n",
        "\n",
        "# Show quick text summary\n",
        "print(\"=== Summary by model ===\")\n",
        "print(summary.round(3))\n",
        "print(\"\\n=== Per-image Overall (wide) ===\")\n",
        "print(wide_overall.round(3).fillna(\"—\"))\n"
      ],
      "metadata": {
        "id": "wi2Siqkq3C6d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}